{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mquanbo-li\u001b[0m (\u001b[33mtwo-phase\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\mycode\\pykan\\torchkan-main\\wandb\\run-20240622_190048-pc5p0b1a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/two-phase/kan/runs/pc5p0b1a' target=\"_blank\">ruby-bird-2</a></strong> to <a href='https://wandb.ai/two-phase/kan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/two-phase/kan' target=\"_blank\">https://wandb.ai/two-phase/kan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/two-phase/kan/runs/pc5p0b1a' target=\"_blank\">https://wandb.ai/two-phase/kan/runs/pc5p0b1a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/two-phase/kan/runs/pc5p0b1a?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x19fcd4c12d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "from torchkan import KAN\n",
    "from KACnet import KAC_Net\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "wandb.init(project=\"kan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, epochs, learning_rate, train_loader, val_loader, model_name):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predicted_y = model(x)\n",
    "            loss = loss_fn(predicted_y, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        #wandb.log({f\"{model_name} Train Loss\": avg_loss})\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                predicted_y = model(x)\n",
    "                val_loss = loss_fn(predicted_y, y.unsqueeze(1))\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        #wandb.log({f\"{model_name} Validation Loss\": avg_val_loss})\n",
    "        print(f\"Epoch {epoch}, {model_name} Train Loss: {avg_loss}, Validation Loss: {avg_val_loss}\")\n",
    "# Evaluation function\n",
    "def evaluate_model(model, eval_loader, model_name):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in eval_loader:\n",
    "            predicted_y = model(x)\n",
    "            predictions.extend(predicted_y.squeeze().cpu().numpy())\n",
    "            actuals.extend(y.cpu().numpy())\n",
    "    return predictions, actuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MLP model matching KAN structure\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        mlp_layers = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            mlp_layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            if i < len(layers) - 2:\n",
    "                mlp_layers.append(nn.ReLU())\n",
    "        self.model = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KAN(\n",
       "  (base_activation): GELU(approximate='none')\n",
       "  (base_weights): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 64x4]\n",
       "      (1): Parameter containing: [torch.float32 of size 64x64]\n",
       "      (2): Parameter containing: [torch.float32 of size 32x64]\n",
       "      (3): Parameter containing: [torch.float32 of size 32x32]\n",
       "      (4): Parameter containing: [torch.float32 of size 16x32]\n",
       "      (5): Parameter containing: [torch.float32 of size 2x16]\n",
       "  )\n",
       "  (spline_weights): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 64x4x8]\n",
       "      (1): Parameter containing: [torch.float32 of size 64x64x8]\n",
       "      (2): Parameter containing: [torch.float32 of size 32x64x8]\n",
       "      (3): Parameter containing: [torch.float32 of size 32x32x8]\n",
       "      (4): Parameter containing: [torch.float32 of size 16x32x8]\n",
       "      (5): Parameter containing: [torch.float32 of size 2x16x8]\n",
       "  )\n",
       "  (layer_norms): ModuleList(\n",
       "    (0-1): 2 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (2-3): 2 x LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (prelus): ModuleList(\n",
       "    (0-5): 6 x PReLU(num_parameters=1)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers=[4, 64, 64, 32, 32, 16, 2]\n",
    "kan_model = KAN(layers)\n",
    "display(kan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数: 307\n",
      "测试集样本数: 77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取xlsx文件\n",
    "file_path = 'D:\\OneDrive - Officials\\OneDrive - Mraz Cindy\\done\\毕设资料\\计算公式说明\\数据库.xlsx'  # 替换为你的xlsx文件路径\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# 打乱行顺序并按8:2的比例分成训练集和测试集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# 提取第2，3，4，5列的数据\n",
    "input = train_df.iloc[:, 1:5]\n",
    "output = train_df.iloc[:, 5:7]\n",
    "test_input = test_df.iloc[:, 1:5]\n",
    "test_label = test_df.iloc[:, 5:7]\n",
    "# 将DataFrame转换为numpy数组并调整其维度为4\n",
    "array1 = input.to_numpy()\n",
    "array2 = output.to_numpy()\n",
    "array3 = test_input.to_numpy()\n",
    "array4 = test_label.to_numpy()\n",
    "input=torch.tensor(array1, dtype=torch.float32)\n",
    "ouput=torch.tensor(array2, dtype=torch.float32)\n",
    "test_input=torch.tensor(array3, dtype=torch.float32)\n",
    "test_label=torch.tensor(array4, dtype=torch.float32)\n",
    "def normalize_columns(tensor):\n",
    "    # 确保输入是2D张量\n",
    "    assert tensor.dim() == 2, \"Input tensor must be 2D\"\n",
    "    \n",
    "    # 获取最小值和最大值\n",
    "    col_min = tensor.min(dim=0, keepdim=True).values\n",
    "    col_max = tensor.max(dim=0, keepdim=True).values\n",
    "    \n",
    "    # 防止除以零的情况\n",
    "    denom = col_max - col_min\n",
    "    denom[denom == 0] = 1  # 如果列中所有值相等，避免除以零\n",
    "    \n",
    "    # 进行归一化\n",
    "    normalized_tensor = (tensor - col_min) / denom\n",
    "    return normalized_tensor\n",
    "# 对每一列进行归一化\n",
    "input= normalize_columns(input)\n",
    "ouput= normalize_columns(ouput)\n",
    "test_input= normalize_columns(test_input)\n",
    "test_label= normalize_columns(test_label)\n",
    "dataset={'train_input':input,'test_input':test_input,'train_label':ouput,'test_label':test_label}\n",
    "\n",
    "# 输出结果\n",
    "print(\"训练集样本数:\", len(train_df))\n",
    "print(\"测试集样本数:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KAC_Net(\n",
       "  (base_activation): SiLU()\n",
       "  (base_weights): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 64x4]\n",
       "      (1): Parameter containing: [torch.float32 of size 64x64]\n",
       "      (2): Parameter containing: [torch.float32 of size 32x64]\n",
       "      (3): Parameter containing: [torch.float32 of size 32x32]\n",
       "      (4): Parameter containing: [torch.float32 of size 16x32]\n",
       "      (5): Parameter containing: [torch.float32 of size 1x16]\n",
       "  )\n",
       "  (poly_weights): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 64x16]\n",
       "      (1): Parameter containing: [torch.float32 of size 64x256]\n",
       "      (2): Parameter containing: [torch.float32 of size 32x256]\n",
       "      (3): Parameter containing: [torch.float32 of size 32x128]\n",
       "      (4): Parameter containing: [torch.float32 of size 16x128]\n",
       "      (5): Parameter containing: [torch.float32 of size 1x64]\n",
       "  )\n",
       "  (layer_norms): ModuleList(\n",
       "    (0-1): 2 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (2-3): 2 x LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers1=[4, 64, 64, 32, 32, 16, 1]\n",
    "KAC_Net=KAC_Net(layers1)\n",
    "display(KAC_Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7454, 0.2318],\n",
      "        [0.4363, 0.4717],\n",
      "        [0.3810, 0.2224],\n",
      "        [0.4316, 0.0911],\n",
      "        [0.4478, 0.2405],\n",
      "        [0.6133, 0.6856],\n",
      "        [0.3451, 0.1812],\n",
      "        [0.4035, 0.2383],\n",
      "        [0.3175, 0.7977],\n",
      "        [0.6342, 0.9059],\n",
      "        [0.3399, 0.1814],\n",
      "        [0.7746, 0.1186],\n",
      "        [0.2827, 0.4222],\n",
      "        [0.3195, 0.3665],\n",
      "        [0.2583, 0.0472],\n",
      "        [0.7440, 0.1156],\n",
      "        [0.3376, 0.4244],\n",
      "        [0.2591, 0.0464],\n",
      "        [0.2664, 0.0479],\n",
      "        [0.2768, 0.0957],\n",
      "        [0.2962, 0.5465],\n",
      "        [0.4832, 0.5239],\n",
      "        [0.2100, 0.0648],\n",
      "        [0.3685, 0.1695],\n",
      "        [0.3592, 0.2739],\n",
      "        [0.2897, 0.7471],\n",
      "        [0.9985, 0.5095],\n",
      "        [0.6502, 0.1128],\n",
      "        [0.3115, 0.0796],\n",
      "        [0.0000, 0.5281],\n",
      "        [0.8494, 0.2458],\n",
      "        [0.8640, 0.6344],\n",
      "        [0.2770, 0.1343],\n",
      "        [0.2988, 0.1089],\n",
      "        [0.1976, 0.0390],\n",
      "        [0.3084, 0.1427],\n",
      "        [0.7358, 0.0826],\n",
      "        [0.2362, 0.0954],\n",
      "        [0.5543, 0.6596],\n",
      "        [0.3370, 0.0925],\n",
      "        [0.8850, 0.1732],\n",
      "        [0.3342, 0.3165],\n",
      "        [0.6512, 0.2230],\n",
      "        [0.3693, 0.1834],\n",
      "        [0.2572, 0.2799],\n",
      "        [0.4290, 0.8704],\n",
      "        [0.3380, 0.2147],\n",
      "        [0.3239, 0.1849],\n",
      "        [0.2368, 0.4683],\n",
      "        [0.7803, 0.2423],\n",
      "        [0.4613, 0.3647],\n",
      "        [0.2784, 0.0941],\n",
      "        [0.3504, 0.1416],\n",
      "        [0.5505, 0.2712],\n",
      "        [0.7770, 0.9559],\n",
      "        [0.3312, 0.1687],\n",
      "        [0.7813, 0.9608],\n",
      "        [0.8436, 0.0824],\n",
      "        [0.4148, 0.8367],\n",
      "        [0.3451, 0.2181],\n",
      "        [0.7382, 0.9314],\n",
      "        [0.4307, 0.3518],\n",
      "        [0.2157, 0.0645],\n",
      "        [0.6954, 0.1629],\n",
      "        [0.2606, 0.1421],\n",
      "        [0.5810, 0.0748],\n",
      "        [0.3821, 0.8177],\n",
      "        [0.3902, 0.3751],\n",
      "        [0.2609, 0.0479],\n",
      "        [0.8893, 0.2472],\n",
      "        [0.2993, 0.1104],\n",
      "        [0.8197, 0.1205],\n",
      "        [0.2242, 0.1885],\n",
      "        [0.3555, 0.2747],\n",
      "        [0.2831, 0.1948],\n",
      "        [0.3961, 0.3803],\n",
      "        [0.3129, 0.1610],\n",
      "        [0.3303, 0.4314],\n",
      "        [0.2646, 0.0622],\n",
      "        [0.9495, 0.5004],\n",
      "        [0.6125, 0.1654],\n",
      "        [0.4202, 0.3628],\n",
      "        [0.2887, 0.0611],\n",
      "        [0.7215, 0.1164],\n",
      "        [0.7863, 0.1693],\n",
      "        [0.5477, 0.1518],\n",
      "        [0.3230, 0.0933],\n",
      "        [0.6103, 0.5127],\n",
      "        [0.2485, 0.1116],\n",
      "        [0.3818, 0.2767],\n",
      "        [0.2305, 0.0649],\n",
      "        [0.2024, 0.0391],\n",
      "        [0.2899, 0.0518],\n",
      "        [0.5504, 0.2120],\n",
      "        [0.2902, 0.0940],\n",
      "        [0.2874, 0.0764],\n",
      "        [0.4620, 0.8884],\n",
      "        [0.2982, 0.1138],\n",
      "        [0.4002, 0.3403],\n",
      "        [0.3506, 0.4358],\n",
      "        [0.9269, 0.9936],\n",
      "        [0.2719, 0.0949],\n",
      "        [0.2159, 0.1494],\n",
      "        [0.7974, 0.3201],\n",
      "        [0.2671, 0.0627],\n",
      "        [0.3431, 0.3772],\n",
      "        [0.2741, 0.2092],\n",
      "        [0.9045, 0.6355],\n",
      "        [0.7953, 0.3109],\n",
      "        [0.2899, 0.3138],\n",
      "        [0.2357, 0.3748],\n",
      "        [0.9185, 0.2528],\n",
      "        [0.3050, 0.7402],\n",
      "        [0.3945, 0.3469],\n",
      "        [0.3562, 0.4430],\n",
      "        [0.7942, 0.6240],\n",
      "        [0.8414, 0.1728],\n",
      "        [0.2259, 0.0394],\n",
      "        [0.1703, 0.0407],\n",
      "        [0.7579, 0.4466],\n",
      "        [0.3518, 0.1410],\n",
      "        [0.3106, 0.1802],\n",
      "        [0.3071, 0.1127],\n",
      "        [0.3107, 0.1449],\n",
      "        [0.2839, 0.7445],\n",
      "        [0.6146, 0.4167],\n",
      "        [0.7489, 0.1650],\n",
      "        [0.2700, 0.7306],\n",
      "        [0.8481, 0.9672],\n",
      "        [0.3364, 0.2788],\n",
      "        [0.2297, 0.7287],\n",
      "        [0.2932, 0.1444],\n",
      "        [0.9881, 0.3399],\n",
      "        [0.3996, 0.1259],\n",
      "        [0.3150, 0.1663],\n",
      "        [0.2993, 0.1104],\n",
      "        [0.2341, 0.1785],\n",
      "        [0.4394, 0.8559],\n",
      "        [0.1347, 0.0464],\n",
      "        [0.6677, 0.2920],\n",
      "        [0.3118, 0.1142],\n",
      "        [0.3664, 0.3701],\n",
      "        [0.3839, 0.2283],\n",
      "        [0.2173, 0.0389],\n",
      "        [0.7726, 0.1716],\n",
      "        [0.6138, 0.5522],\n",
      "        [0.6847, 0.5488],\n",
      "        [0.3073, 0.3693],\n",
      "        [0.4015, 0.8528],\n",
      "        [0.3414, 0.1190],\n",
      "        [0.2525, 0.0623],\n",
      "        [0.3624, 0.3717],\n",
      "        [0.2014, 0.0396],\n",
      "        [0.2398, 0.0645],\n",
      "        [0.3417, 0.3723],\n",
      "        [0.2362, 0.0954],\n",
      "        [0.9457, 0.3328],\n",
      "        [1.0000, 0.6752],\n",
      "        [0.3592, 0.1244],\n",
      "        [0.4333, 0.1835],\n",
      "        [0.3777, 0.2779],\n",
      "        [0.2427, 0.0384],\n",
      "        [0.3328, 0.7911],\n",
      "        [0.2341, 0.0625],\n",
      "        [0.4015, 0.2764],\n",
      "        [0.3218, 0.0831],\n",
      "        [0.2338, 0.2802],\n",
      "        [0.9403, 0.2548],\n",
      "        [0.2618, 0.0754],\n",
      "        [0.6942, 0.2291],\n",
      "        [0.6337, 0.0769],\n",
      "        [0.2065, 0.0407],\n",
      "        [0.2865, 0.0476],\n",
      "        [0.3274, 0.1409],\n",
      "        [0.4152, 0.1353],\n",
      "        [0.3292, 0.3660],\n",
      "        [0.3862, 0.2799],\n",
      "        [0.6056, 0.2179],\n",
      "        [0.8944, 0.3215],\n",
      "        [0.3224, 0.1828],\n",
      "        [0.2699, 0.0624],\n",
      "        [0.6008, 0.0463],\n",
      "        [0.5555, 0.4013],\n",
      "        [0.4781, 0.3920],\n",
      "        [0.2936, 0.4307],\n",
      "        [0.4047, 0.1776],\n",
      "        [0.2233, 0.0394],\n",
      "        [0.6548, 0.5626],\n",
      "        [0.2923, 0.1830],\n",
      "        [0.3036, 0.2747],\n",
      "        [0.8027, 0.6291],\n",
      "        [0.2468, 0.0000],\n",
      "        [0.3237, 0.4674],\n",
      "        [0.3698, 0.8254],\n",
      "        [0.2660, 0.0719],\n",
      "        [0.4196, 0.2320],\n",
      "        [0.6084, 0.6487],\n",
      "        [0.7511, 0.2995],\n",
      "        [0.7899, 0.2402],\n",
      "        [0.1929, 0.1032],\n",
      "        [0.3027, 0.1806],\n",
      "        [0.2743, 0.0725],\n",
      "        [0.2255, 0.0394],\n",
      "        [0.2194, 0.0390],\n",
      "        [0.7986, 0.4639],\n",
      "        [0.2381, 0.0993],\n",
      "        [0.2532, 0.5007],\n",
      "        [0.7583, 0.0803],\n",
      "        [0.2876, 0.3692],\n",
      "        [0.3612, 0.1837],\n",
      "        [0.3155, 0.2789],\n",
      "        [0.5551, 0.5315],\n",
      "        [0.7567, 0.2323],\n",
      "        [0.3855, 0.3762],\n",
      "        [0.9323, 0.1781],\n",
      "        [0.2226, 0.0645],\n",
      "        [0.9139, 0.1771],\n",
      "        [0.2876, 0.0942],\n",
      "        [0.1528, 0.0435],\n",
      "        [0.2781, 0.0752],\n",
      "        [0.1873, 0.0411],\n",
      "        [0.6579, 0.2259],\n",
      "        [0.3127, 0.0523],\n",
      "        [0.6850, 0.9215],\n",
      "        [0.3328, 0.4388],\n",
      "        [0.7857, 0.3158],\n",
      "        [0.9294, 0.4904],\n",
      "        [0.4577, 0.5019],\n",
      "        [0.3885, 0.1822],\n",
      "        [0.6978, 0.2952],\n",
      "        [0.3019, 0.1849],\n",
      "        [0.3330, 0.4240],\n",
      "        [0.3403, 0.5324],\n",
      "        [0.2833, 0.2744],\n",
      "        [0.3236, 0.7510],\n",
      "        [0.3109, 0.4637],\n",
      "        [0.7021, 0.5794],\n",
      "        [0.3343, 0.2759],\n",
      "        [0.6575, 0.1583],\n",
      "        [0.3000, 0.1413],\n",
      "        [0.3322, 0.0550],\n",
      "        [0.3877, 0.2231],\n",
      "        [0.4478, 0.1971],\n",
      "        [0.7436, 0.5901],\n",
      "        [0.3524, 0.3297],\n",
      "        [0.6575, 0.1583],\n",
      "        [0.1853, 0.0261],\n",
      "        [0.2135, 0.0476],\n",
      "        [0.2364, 0.0748],\n",
      "        [0.2526, 0.0625],\n",
      "        [0.3266, 0.2087],\n",
      "        [0.3046, 0.0506],\n",
      "        [0.3136, 0.2718],\n",
      "        [0.3715, 0.0868],\n",
      "        [0.2568, 0.0311],\n",
      "        [0.7941, 0.4712],\n",
      "        [0.2795, 0.3145],\n",
      "        [0.2542, 0.0453],\n",
      "        [0.7960, 0.6158],\n",
      "        [0.3553, 0.0838],\n",
      "        [0.8631, 0.4789],\n",
      "        [0.3081, 0.1668],\n",
      "        [0.4670, 0.4871],\n",
      "        [0.3292, 0.7494],\n",
      "        [0.3111, 0.0943],\n",
      "        [0.7571, 0.5920],\n",
      "        [0.3018, 0.7417],\n",
      "        [0.3991, 0.4654],\n",
      "        [0.6123, 0.3752],\n",
      "        [0.5983, 0.1094],\n",
      "        [0.1564, 0.0694],\n",
      "        [0.7934, 0.2470],\n",
      "        [0.9843, 0.2585],\n",
      "        [0.9844, 1.0000],\n",
      "        [0.4346, 0.1886],\n",
      "        [0.3725, 0.1712],\n",
      "        [0.2693, 0.0758],\n",
      "        [0.5414, 0.1071],\n",
      "        [0.2628, 0.4694],\n",
      "        [0.4430, 0.2464],\n",
      "        [0.6710, 0.4331],\n",
      "        [0.6974, 0.0793],\n",
      "        [0.4058, 0.4564],\n",
      "        [0.3248, 0.1660],\n",
      "        [0.9038, 0.4801],\n",
      "        [0.3561, 0.1426],\n",
      "        [0.2501, 0.3949],\n",
      "        [0.3356, 0.5309],\n",
      "        [0.2603, 0.4309],\n",
      "        [0.3210, 0.3751],\n",
      "        [0.2251, 0.1230],\n",
      "        [0.3716, 0.1212],\n",
      "        [0.3157, 0.1623],\n",
      "        [0.2625, 0.7321],\n",
      "        [0.6080, 0.0930],\n",
      "        [0.8970, 0.1241],\n",
      "        [0.9486, 0.6650],\n",
      "        [0.3203, 0.2161],\n",
      "        [0.3404, 0.1164],\n",
      "        [0.3357, 0.7543],\n",
      "        [0.1991, 0.0411],\n",
      "        [0.3752, 0.1407],\n",
      "        [0.2342, 0.0626],\n",
      "        [0.2982, 0.1138],\n",
      "        [0.8019, 0.4753],\n",
      "        [0.6692, 0.7111],\n",
      "        [0.6067, 0.1547]])\n"
     ]
    }
   ],
   "source": [
    "print(ouput\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pc5p0b1a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73dbe48aa31a483a8759120c922da689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.041 MB of 0.055 MB uploaded\\r'), FloatProgress(value=0.7411443199114433, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-bird-2</strong> at: <a href='https://wandb.ai/two-phase/kan/runs/pc5p0b1a' target=\"_blank\">https://wandb.ai/two-phase/kan/runs/pc5p0b1a</a><br/> View project at: <a href='https://wandb.ai/two-phase/kan' target=\"_blank\">https://wandb.ai/two-phase/kan</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240622_190048-pc5p0b1a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pc5p0b1a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\mycode\\pykan\\torchkan-main\\wandb\\run-20240622_190153-zvv7w4pa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/two-phase/kan/runs/zvv7w4pa' target=\"_blank\">trim-bird-3</a></strong> to <a href='https://wandb.ai/two-phase/kan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/two-phase/kan' target=\"_blank\">https://wandb.ai/two-phase/kan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/two-phase/kan/runs/zvv7w4pa' target=\"_blank\">https://wandb.ai/two-phase/kan/runs/zvv7w4pa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, KAN Train Loss: 0.42429396841261124, Validation Loss: 0.36561378836631775\n",
      "Epoch 1, KAN Train Loss: 0.37964848677317303, Validation Loss: 0.35841965675354004\n",
      "Epoch 2, KAN Train Loss: 0.34265784753693473, Validation Loss: 0.3250409960746765\n",
      "Epoch 3, KAN Train Loss: 0.31973867615063983, Validation Loss: 0.3107745796442032\n",
      "Epoch 4, KAN Train Loss: 0.3024112814002567, Validation Loss: 0.2973761707544327\n",
      "Epoch 5, KAN Train Loss: 0.28700319760375553, Validation Loss: 0.27531349658966064\n",
      "Epoch 6, KAN Train Loss: 0.26408912738164264, Validation Loss: 0.25749843567609787\n",
      "Epoch 7, KAN Train Loss: 0.24823783006933, Validation Loss: 0.2453388012945652\n",
      "Epoch 8, KAN Train Loss: 0.23966077963511148, Validation Loss: 0.2339014895260334\n",
      "Epoch 9, KAN Train Loss: 0.22208855715062883, Validation Loss: 0.22336682677268982\n",
      "Epoch 10, KAN Train Loss: 0.22161036729812622, Validation Loss: 0.21842137724161148\n",
      "Epoch 11, KAN Train Loss: 0.215628981590271, Validation Loss: 0.21358750760555267\n",
      "Epoch 12, KAN Train Loss: 0.21461057331826952, Validation Loss: 0.20888512581586838\n",
      "Epoch 13, KAN Train Loss: 0.20632212195131513, Validation Loss: 0.20434635877609253\n",
      "Epoch 14, KAN Train Loss: 0.20530631144841513, Validation Loss: 0.1999150589108467\n",
      "Epoch 15, KAN Train Loss: 0.199157843987147, Validation Loss: 0.1956297717988491\n",
      "Epoch 16, KAN Train Loss: 0.194653214679824, Validation Loss: 0.1914658583700657\n",
      "Epoch 17, KAN Train Loss: 0.18712849583890703, Validation Loss: 0.1874421313405037\n",
      "Epoch 18, KAN Train Loss: 0.18293208049403298, Validation Loss: 0.18359747156500816\n",
      "Epoch 19, KAN Train Loss: 0.1807926760779487, Validation Loss: 0.17988139390945435\n",
      "Epoch 20, KAN Train Loss: 0.1766856180297004, Validation Loss: 0.17808464914560318\n",
      "Epoch 21, KAN Train Loss: 0.18359837101565468, Validation Loss: 0.1763097420334816\n",
      "Epoch 22, KAN Train Loss: 0.1722904841105143, Validation Loss: 0.17454428225755692\n",
      "Epoch 23, KAN Train Loss: 0.16924853374560675, Validation Loss: 0.17280753329396248\n",
      "Epoch 24, KAN Train Loss: 0.17039822041988373, Validation Loss: 0.17110517621040344\n",
      "Epoch 25, KAN Train Loss: 0.16797692080338797, Validation Loss: 0.1693989783525467\n",
      "Epoch 26, KAN Train Loss: 0.1697089953555001, Validation Loss: 0.16771230474114418\n",
      "Epoch 27, KAN Train Loss: 0.16150819013516107, Validation Loss: 0.16606798395514488\n",
      "Epoch 28, KAN Train Loss: 0.16519948674572837, Validation Loss: 0.1644630767405033\n",
      "Epoch 29, KAN Train Loss: 0.16207936074998644, Validation Loss: 0.1628572940826416\n",
      "Epoch 30, KAN Train Loss: 0.15994885729418862, Validation Loss: 0.16205810755491257\n",
      "Epoch 31, KAN Train Loss: 0.1585195834438006, Validation Loss: 0.16127778962254524\n",
      "Epoch 32, KAN Train Loss: 0.15663383238845402, Validation Loss: 0.16050605475902557\n",
      "Epoch 33, KAN Train Loss: 0.1585853464073605, Validation Loss: 0.15973654761910439\n",
      "Epoch 34, KAN Train Loss: 0.15927337772316402, Validation Loss: 0.15895549952983856\n",
      "Epoch 35, KAN Train Loss: 0.15300226625468996, Validation Loss: 0.15817594528198242\n",
      "Epoch 36, KAN Train Loss: 0.1584053503142463, Validation Loss: 0.15741170197725296\n",
      "Epoch 37, KAN Train Loss: 0.15312506589624617, Validation Loss: 0.15664756298065186\n",
      "Epoch 38, KAN Train Loss: 0.15473910007211897, Validation Loss: 0.15588683262467384\n",
      "Epoch 39, KAN Train Loss: 0.15766719977060953, Validation Loss: 0.15513154864311218\n",
      "Epoch 40, KAN Train Loss: 0.1559496753745609, Validation Loss: 0.1547509916126728\n",
      "Epoch 41, KAN Train Loss: 0.15165874196423423, Validation Loss: 0.15437361225485802\n",
      "Epoch 42, KAN Train Loss: 0.15139143500063154, Validation Loss: 0.15399785712361336\n",
      "Epoch 43, KAN Train Loss: 0.15185913112428454, Validation Loss: 0.15362249687314034\n",
      "Epoch 44, KAN Train Loss: 0.15699657715029186, Validation Loss: 0.15324728563427925\n",
      "Epoch 45, KAN Train Loss: 0.15661339792940351, Validation Loss: 0.15287264809012413\n",
      "Epoch 46, KAN Train Loss: 0.15289022773504257, Validation Loss: 0.15249527990818024\n",
      "Epoch 47, KAN Train Loss: 0.15363218718104893, Validation Loss: 0.15212108194828033\n",
      "Epoch 48, KAN Train Loss: 0.1520439369810952, Validation Loss: 0.1517485361546278\n",
      "Epoch 49, KAN Train Loss: 0.15140502651532492, Validation Loss: 0.15137138217687607\n",
      "Epoch 0, MLP Train Loss: 0.20039772656228808, Validation Loss: 0.17659588530659676\n",
      "Epoch 1, MLP Train Loss: 0.1697349813249376, Validation Loss: 0.14946148730814457\n",
      "Epoch 2, MLP Train Loss: 0.13638781093888813, Validation Loss: 0.11044976487755775\n",
      "Epoch 3, MLP Train Loss: 0.09296252619889048, Validation Loss: 0.06236962974071503\n",
      "Epoch 4, MLP Train Loss: 0.06259980632199182, Validation Loss: 0.0655499380081892\n",
      "Epoch 5, MLP Train Loss: 0.06004041598902808, Validation Loss: 0.05769324488937855\n",
      "Epoch 6, MLP Train Loss: 0.05887673505478435, Validation Loss: 0.05817085783928633\n",
      "Epoch 7, MLP Train Loss: 0.059281048675378166, Validation Loss: 0.057339541614055634\n",
      "Epoch 8, MLP Train Loss: 0.0570398254526986, Validation Loss: 0.05795285012573004\n",
      "Epoch 9, MLP Train Loss: 0.056778220666779414, Validation Loss: 0.05689833126962185\n",
      "Epoch 10, MLP Train Loss: 0.05905566902624236, Validation Loss: 0.056855146773159504\n",
      "Epoch 11, MLP Train Loss: 0.0607303095360597, Validation Loss: 0.056827676482498646\n",
      "Epoch 12, MLP Train Loss: 0.05811868401037322, Validation Loss: 0.05694179888814688\n",
      "Epoch 13, MLP Train Loss: 0.05683576770954662, Validation Loss: 0.05700651276856661\n",
      "Epoch 14, MLP Train Loss: 0.05730223283171654, Validation Loss: 0.05675358325242996\n",
      "Epoch 15, MLP Train Loss: 0.05705089991291364, Validation Loss: 0.05664739664644003\n",
      "Epoch 16, MLP Train Loss: 0.057027889208661184, Validation Loss: 0.056629979982972145\n",
      "Epoch 17, MLP Train Loss: 0.05921365444858869, Validation Loss: 0.056664228439331055\n",
      "Epoch 18, MLP Train Loss: 0.05783002202709516, Validation Loss: 0.056768473237752914\n",
      "Epoch 19, MLP Train Loss: 0.05657051710618867, Validation Loss: 0.05669779051095247\n",
      "Epoch 20, MLP Train Loss: 0.057282313290569514, Validation Loss: 0.05662325769662857\n",
      "Epoch 21, MLP Train Loss: 0.057342789239353605, Validation Loss: 0.05662316735833883\n",
      "Epoch 22, MLP Train Loss: 0.05447853853305181, Validation Loss: 0.056561753153800964\n",
      "Epoch 23, MLP Train Loss: 0.05718299042847422, Validation Loss: 0.056552681140601635\n",
      "Epoch 24, MLP Train Loss: 0.05683025883303748, Validation Loss: 0.0565748130902648\n",
      "Epoch 25, MLP Train Loss: 0.05907665896746847, Validation Loss: 0.05651363357901573\n",
      "Epoch 26, MLP Train Loss: 0.05560833712418874, Validation Loss: 0.05659588798880577\n",
      "Epoch 27, MLP Train Loss: 0.0575422810183631, Validation Loss: 0.0565547002479434\n",
      "Epoch 28, MLP Train Loss: 0.0581143523255984, Validation Loss: 0.05654572416096926\n",
      "Epoch 29, MLP Train Loss: 0.05514067908128103, Validation Loss: 0.056549147702753544\n",
      "Epoch 30, MLP Train Loss: 0.05481580686238077, Validation Loss: 0.056552925147116184\n",
      "Epoch 31, MLP Train Loss: 0.05716714676883486, Validation Loss: 0.05651748180389404\n",
      "Epoch 32, MLP Train Loss: 0.05601589795615938, Validation Loss: 0.05653087142854929\n",
      "Epoch 33, MLP Train Loss: 0.05683156930738025, Validation Loss: 0.056504737585783005\n",
      "Epoch 34, MLP Train Loss: 0.05542369931936264, Validation Loss: 0.05649872496724129\n",
      "Epoch 35, MLP Train Loss: 0.055379745033052236, Validation Loss: 0.05651452671736479\n",
      "Epoch 36, MLP Train Loss: 0.0572035014629364, Validation Loss: 0.05653970129787922\n",
      "Epoch 37, MLP Train Loss: 0.056969391802946724, Validation Loss: 0.05657374206930399\n",
      "Epoch 38, MLP Train Loss: 0.056600922925604716, Validation Loss: 0.056585827842354774\n",
      "Epoch 39, MLP Train Loss: 0.05478920001122686, Validation Loss: 0.05653079506009817\n",
      "Epoch 40, MLP Train Loss: 0.057978869726260505, Validation Loss: 0.056517272256314754\n",
      "Epoch 41, MLP Train Loss: 0.057107708934280604, Validation Loss: 0.05652310512959957\n",
      "Epoch 42, MLP Train Loss: 0.05672002459565798, Validation Loss: 0.056526645086705685\n",
      "Epoch 43, MLP Train Loss: 0.05648583215143946, Validation Loss: 0.05651833303272724\n",
      "Epoch 44, MLP Train Loss: 0.057287261303928166, Validation Loss: 0.05654046777635813\n",
      "Epoch 45, MLP Train Loss: 0.05486950361066394, Validation Loss: 0.05652120057493448\n",
      "Epoch 46, MLP Train Loss: 0.0548896806107627, Validation Loss: 0.05651306640356779\n",
      "Epoch 47, MLP Train Loss: 0.05916492227050993, Validation Loss: 0.05651955306529999\n",
      "Epoch 48, MLP Train Loss: 0.06038703935013877, Validation Loss: 0.05655109230428934\n",
      "Epoch 49, MLP Train Loss: 0.05728216179543071, Validation Loss: 0.056554156355559826\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 1314] 客户端没有所需的特权。: 'f:\\\\mycode\\\\pykan\\\\torchkan-main\\\\kan inverse.pth' -> 'f:\\\\mycode\\\\pykan\\\\torchkan-main\\\\wandb\\\\run-20240622_190153-zvv7w4pa\\\\files\\\\kan inverse.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(kan_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkan inverse.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(mlp_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlp inverse.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkan inverse.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m wandb\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlp inverse.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:400\u001b[0m, in \u001b[0;36m_run_decorator._noop_on_finish.<locals>.decorator_fn.<locals>.wrapper_fn\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_finished\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 400\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m     default_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is finished. The call to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be ignored. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure that you are using an active run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    406\u001b[0m     resolved_message \u001b[38;5;241m=\u001b[39m message \u001b[38;5;129;01mor\u001b[39;00m default_message\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:390\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1981\u001b[0m, in \u001b[0;36mRun.save\u001b[1;34m(self, glob_str, base_path, policy)\u001b[0m\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1976\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlive\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnow\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m policies are currently supported.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1977\u001b[0m     )\n\u001b[0;32m   1979\u001b[0m resolved_base_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPurePath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(base_path))\n\u001b[1;32m-> 1981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_glob_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_base_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2045\u001b[0m, in \u001b[0;36mRun._save\u001b[1;34m(self, glob_path, base_path, policy)\u001b[0m\n\u001b[0;32m   2040\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;66;03m# In Python 3.8, we would pass missing_ok=True, but as of now\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m         \u001b[38;5;66;03m# we support down to Python 3.7.\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m-> 2045\u001b[0m     \u001b[43mtarget_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymlink_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;66;03m# Inform users that new files aren't detected automatically.\u001b[39;00m\n\u001b[0;32m   2048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m had_symlinked_files \u001b[38;5;129;01mand\u001b[39;00m is_star_glob:\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\pathlib.py:1198\u001b[0m, in \u001b[0;36mPath.symlink_to\u001b[1;34m(self, target, target_is_directory)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(os, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymlink\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mos.symlink() not available on this system\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1198\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_is_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1314] 客户端没有所需的特权。: 'f:\\\\mycode\\\\pykan\\\\torchkan-main\\\\kan inverse.pth' -> 'f:\\\\mycode\\\\pykan\\\\torchkan-main\\\\wandb\\\\run-20240622_190153-zvv7w4pa\\\\files\\\\kan inverse.pth'"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"kan\")\n",
    "dimension=4\n",
    "# Define model layers\n",
    "layers = [dimension, 64, 64, 32, 32, 16, 2]\n",
    "x_data=torch.cat((input,test_input),0)\n",
    "y_data=torch.cat((ouput,test_label),0)\n",
    "\n",
    "# 定义一个TensorDataset对象，将x_data和y_data传入\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "# 计算训练集和验证集的数量\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# 将数据集划分为训练集和验证集\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# 创建训练集的DataLoader对象，批量大小为32，打乱数据\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# 创建验证集的DataLoader对象，批量大小为32，不打乱数据\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize and train the KAN model\n",
    "kan_model = KAN(layers)\n",
    "train_and_validate_model(kan_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"KAN\")\n",
    "\n",
    "    # Initialize and train the MLP model\n",
    "mlp_model = MLP(layers)\n",
    "train_and_validate_model(mlp_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"MLP\")\n",
    "\n",
    "    # Evaluate both models\n",
    "kan_predictions, kan_actuals = evaluate_model(kan_model, val_loader, f\"KAN\")\n",
    "mlp_predictions, mlp_actuals = evaluate_model(mlp_model, val_loader, f\"MLP\")\n",
    "\n",
    "    # Log results to wandb\n",
    "kan_data = [[pred, act] for pred, act in zip(kan_predictions, kan_actuals)]\n",
    "mlp_data = [[pred, act] for pred, act in zip(mlp_predictions, mlp_actuals)]\n",
    "wandb.log({\n",
    "        f\"KAN Predictions vs Actuals\": wandb.Table(data=kan_data, columns=[\"KAN Predictions\", \"Actuals\"]),\n",
    "        f\"MLP Predictions vs Actuals\": wandb.Table(data=mlp_data, columns=[\"MLP Predictions\", \"Actuals\"])\n",
    "    })\n",
    "\n",
    "    # Save model states\n",
    "torch.save(kan_model.state_dict(), f\"kan inverse.pth\")\n",
    "torch.save(mlp_model.state_dict(), f\"mlp inverse.pth\")\n",
    "wandb.save(f\"kan inverse.pth\")\n",
    "wandb.save(f\"mlp inverse.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
