{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "from torchkan import KAN\n",
    "from KACnet import KAC_Net\n",
    "from KALnet import KAL_Net\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, epochs, learning_rate, train_loader, val_loader, model_name):\n",
    "    '''\n",
    "    这段代码是用Python编写的，主要用于训练一个神经网络模型。以下是代码的详细解释：\n",
    "\n",
    "1. 首先，导入所需的库和模块。这里使用了PyTorch（nn和optim模块）和wandb（用于跟踪训练过程）。\n",
    "\n",
    "2. 定义损失函数（均方误差损失）和优化器（随机梯度下降）。\n",
    "\n",
    "3. 定义学习率调度器（StepLR），用于在每个训练周期后调整学习率。这里设置了步长为10，gamma为0.5，这意味着每10个周期，学习率将乘以0.5。\n",
    "\n",
    "4. 定义训练和验证数据加载器（train_loader和val_loader）。\n",
    "\n",
    "5. 定义训练循环，共进行epochs个周期。在每个周期中：\n",
    "\n",
    "   a. 将模型设置为训练模式（model.train()）。\n",
    "\n",
    "   b. 初始化总损失为0（total_loss = 0）。\n",
    "\n",
    "   c. 遍历训练数据加载器（for x, y in train_loader:），对每个输入（x）和目标（y）进行以下操作：\n",
    "\n",
    "      i. 清除梯度（optimizer.zero_grad()）。\n",
    "/\n",
    "      ii. 使用模型预测输出（predicted_y = model(x)）。\n",
    "\n",
    "      iii. 计算损失（loss = loss_fn(predicted_y, y.unsqueeze(1))）。\n",
    "\n",
    "      iv. 反向传播损失（loss.backward()）。\n",
    "\n",
    "      v. 更新模型参数（optimizer.step()）。\n",
    "\n",
    "      vi. 将损失累加到总损失中（total_loss += loss.item()）。\n",
    "\n",
    "   d. 更新学习率（scheduler.step()）。\n",
    "\n",
    "   e. 计算平均训练损失（avg_loss = total_loss / len(train_loader)），并使用wandb记录训练损失（wandb.log({f\"{model_name} Train Loss\": avg_loss})）。\n",
    "\n",
    "6. 将模型设置为评估模式（model.eval()）。\n",
    "\n",
    "7. 初始化总验证损失为0（total_val_loss = 0）。\n",
    "\n",
    "8. 遍历验证数据加载器（for x, y in val_loader:），对每个输入（x）和目标（y）进行以下操作：\n",
    "\n",
    "   a. 使用模型预测输出（predicted_y = model(x)）。\n",
    "\n",
    "   b. 计算验证损失（val_loss = loss_fn(predicted_y, y.unsqueeze(1))）。\n",
    "\n",
    "   c. 将验证损失累加到总验证损失中（total_val_loss += val_loss.item()）。\n",
    "\n",
    "9. 计算平均验证损失（avg_val_loss = total_val_loss / len(val_loader)），并使用wandb记录验证损失（wandb.log({f\"{model_name} Validation Loss\": avg_val_loss})）。\n",
    "\n",
    "10. 打印训练和验证损失（print(f\"Epoch {epoch}, {model_name} Train Loss: {avg_loss}, Validation Loss: {avg_val_loss}\")）。\n",
    "\n",
    "总之，这段代码实现了一个简单的神经网络模型训练过程，包括训练和验证损失的计算、学习率的调度以及训练过程的记录。\n",
    "\n",
    "'''\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predicted_y = model(x)\n",
    "            loss = loss_fn(predicted_y, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        wandb.log({f\"{model_name} Train Loss\": avg_loss})\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                predicted_y = model(x)\n",
    "                val_loss = loss_fn(predicted_y, y.unsqueeze(1))\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        wandb.log({f\"{model_name} Validation Loss\": avg_val_loss})\n",
    "        print(f\"Epoch {epoch}, {model_name} Train Loss: {avg_loss}, Validation Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义验证模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader, model_name):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in eval_loader:\n",
    "            predicted_y = model(x)\n",
    "            predictions.extend(predicted_y.squeeze().cpu().numpy())\n",
    "            actuals.extend(y.cpu().numpy())\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义mlp模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        mlp_layers = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            mlp_layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            if i < len(layers) - 2:\n",
    "                mlp_layers.append(nn.ReLU())\n",
    "        self.model = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取并清理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数: 307\n",
      "测试集样本数: 77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取xlsx文件\n",
    "file_path = 'D:\\OneDrive - Officials\\OneDrive - Mraz Cindy\\done\\毕设资料\\计算公式说明\\数据库.xlsx'  # 替换为你的xlsx文件路径\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# 打乱行顺序并按8:2的比例分成训练集和测试集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# 提取第2，3，4，5列的数据\n",
    "input = train_df.iloc[:, 1:5]\n",
    "output = train_df.iloc[:, 5:7]\n",
    "test_input = test_df.iloc[:, 1:5]\n",
    "test_label = test_df.iloc[:, 5:7]\n",
    "# 将DataFrame转换为numpy数组并调整其维度为4\n",
    "array1 = input.to_numpy()\n",
    "array2 = output.to_numpy()\n",
    "array3 = test_input.to_numpy()\n",
    "array4 = test_label.to_numpy()\n",
    "input=torch.tensor(array1, dtype=torch.float32)\n",
    "ouput=torch.tensor(array2, dtype=torch.float32)\n",
    "test_input=torch.tensor(array3, dtype=torch.float32)\n",
    "test_label=torch.tensor(array4, dtype=torch.float32)\n",
    "def normalize_columns(tensor):\n",
    "    # 确保输入是2D张量\n",
    "    assert tensor.dim() == 2, \"Input tensor must be 2D\"\n",
    "    \n",
    "    # 获取最小值和最大值\n",
    "    col_min = tensor.min(dim=0, keepdim=True).values\n",
    "    col_max = tensor.max(dim=0, keepdim=True).values\n",
    "    \n",
    "    # 防止除以零的情况\n",
    "    denom = col_max - col_min\n",
    "    denom[denom == 0] = 1  # 如果列中所有值相等，避免除以零\n",
    "    \n",
    "    # 进行归一化\n",
    "    normalized_tensor = (tensor - col_min) / denom\n",
    "    return normalized_tensor\n",
    "# 对每一列进行归一化\n",
    "input= normalize_columns(input)\n",
    "ouput= normalize_columns(ouput)\n",
    "test_input= normalize_columns(test_input)\n",
    "test_label= normalize_columns(test_label)\n",
    "dataset={'train_input':input,'test_input':test_input,'train_label':ouput,'test_label':test_label}\n",
    "\n",
    "# 输出结果\n",
    "print(\"训练集样本数:\", len(train_df))\n",
    "print(\"测试集样本数:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始跑模型并存在wandb里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mquanbo-li\u001b[0m (\u001b[33mtwo-phase\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72aa1d92a234bc0b5daf26186346854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01127777777777131, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\mycode\\pykan\\torchkan-main\\wandb\\run-20240629_172101-ld15wxbn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/two-phase/kan/runs/ld15wxbn' target=\"_blank\">jolly-bush-21</a></strong> to <a href='https://wandb.ai/two-phase/kan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/two-phase/kan' target=\"_blank\">https://wandb.ai/two-phase/kan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/two-phase/kan/runs/ld15wxbn' target=\"_blank\">https://wandb.ai/two-phase/kan/runs/ld15wxbn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\mycode\\pykan\\kannet\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 2])) that is different to the input size (torch.Size([32, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "f:\\mycode\\pykan\\kannet\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([12, 1, 2])) that is different to the input size (torch.Size([12, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "f:\\mycode\\pykan\\kannet\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([20, 1, 2])) that is different to the input size (torch.Size([20, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, KAN Train Loss: 0.36593832572301227, Validation Loss: 0.3772151246666908\n",
      "Epoch 1, KAN Train Loss: 0.34858745998806423, Validation Loss: 0.3600526675581932\n",
      "Epoch 2, KAN Train Loss: 0.31648484534687465, Validation Loss: 0.33948011696338654\n",
      "Epoch 3, KAN Train Loss: 0.3013068503803677, Validation Loss: 0.32770106941461563\n",
      "Epoch 4, KAN Train Loss: 0.290074970987108, Validation Loss: 0.3064395785331726\n",
      "Epoch 5, KAN Train Loss: 0.26734599471092224, Validation Loss: 0.29278596490621567\n",
      "Epoch 6, KAN Train Loss: 0.25508469177616966, Validation Loss: 0.27975212410092354\n",
      "Epoch 7, KAN Train Loss: 0.24385305576854283, Validation Loss: 0.26749658584594727\n",
      "Epoch 8, KAN Train Loss: 0.23190097510814667, Validation Loss: 0.25595897808671\n",
      "Epoch 9, KAN Train Loss: 0.22320434947808585, Validation Loss: 0.24146617203950882\n",
      "Epoch 10, KAN Train Loss: 0.20988880263434517, Validation Loss: 0.23633262887597084\n",
      "Epoch 11, KAN Train Loss: 0.20765690836641523, Validation Loss: 0.23137370496988297\n",
      "Epoch 12, KAN Train Loss: 0.2004329396618737, Validation Loss: 0.2265246920287609\n",
      "Epoch 13, KAN Train Loss: 0.19533077544636196, Validation Loss: 0.22182383760809898\n",
      "Epoch 14, KAN Train Loss: 0.19621275696489546, Validation Loss: 0.2172761633992195\n",
      "Epoch 15, KAN Train Loss: 0.19005968504481846, Validation Loss: 0.21285436674952507\n",
      "Epoch 16, KAN Train Loss: 0.18347376585006714, Validation Loss: 0.20855843648314476\n",
      "Epoch 17, KAN Train Loss: 0.1809734602769216, Validation Loss: 0.20441868156194687\n",
      "Epoch 18, KAN Train Loss: 0.17766476339764065, Validation Loss: 0.2003849409520626\n",
      "Epoch 19, KAN Train Loss: 0.17528081271383497, Validation Loss: 0.1964564360678196\n",
      "Epoch 20, KAN Train Loss: 0.1706927369038264, Validation Loss: 0.19455915689468384\n",
      "Epoch 21, KAN Train Loss: 0.16830642521381378, Validation Loss: 0.19269604608416557\n",
      "Epoch 22, KAN Train Loss: 0.1695710238483217, Validation Loss: 0.19084781780838966\n",
      "Epoch 23, KAN Train Loss: 0.16765358712938097, Validation Loss: 0.18901477009058\n",
      "Epoch 24, KAN Train Loss: 0.16974384751584795, Validation Loss: 0.1872168481349945\n",
      "Epoch 25, KAN Train Loss: 0.1619416442182329, Validation Loss: 0.1854572705924511\n",
      "Epoch 26, KAN Train Loss: 0.16107779244581857, Validation Loss: 0.18371789529919624\n",
      "Epoch 27, KAN Train Loss: 0.16150602367189196, Validation Loss: 0.18199818581342697\n",
      "Epoch 28, KAN Train Loss: 0.15873402605454126, Validation Loss: 0.1803080476820469\n",
      "Epoch 29, KAN Train Loss: 0.16213199496269226, Validation Loss: 0.17862052470445633\n",
      "Epoch 30, KAN Train Loss: 0.1560172157155143, Validation Loss: 0.17778561636805534\n",
      "Epoch 31, KAN Train Loss: 0.15279290162854725, Validation Loss: 0.17694858834147453\n",
      "Epoch 32, KAN Train Loss: 0.15609388964043724, Validation Loss: 0.17611948400735855\n",
      "Epoch 33, KAN Train Loss: 0.15788196358415815, Validation Loss: 0.1752919666469097\n",
      "Epoch 34, KAN Train Loss: 0.1573464655213886, Validation Loss: 0.17446177452802658\n",
      "Epoch 35, KAN Train Loss: 0.1529193421204885, Validation Loss: 0.17363892123103142\n",
      "Epoch 36, KAN Train Loss: 0.15642612841394213, Validation Loss: 0.17281900718808174\n",
      "Epoch 37, KAN Train Loss: 0.15198297633065116, Validation Loss: 0.17200394719839096\n",
      "Epoch 38, KAN Train Loss: 0.15383894741535187, Validation Loss: 0.1712142527103424\n",
      "Epoch 39, KAN Train Loss: 0.14861684872044456, Validation Loss: 0.17042072862386703\n",
      "Epoch 40, KAN Train Loss: 0.1584493029448721, Validation Loss: 0.17002469301223755\n",
      "Epoch 41, KAN Train Loss: 0.15529722968737283, Validation Loss: 0.1696242205798626\n",
      "Epoch 42, KAN Train Loss: 0.1507410423623191, Validation Loss: 0.16922316700220108\n",
      "Epoch 43, KAN Train Loss: 0.14625749902592766, Validation Loss: 0.16882627084851265\n",
      "Epoch 44, KAN Train Loss: 0.14515452335278192, Validation Loss: 0.16843624040484428\n",
      "Epoch 45, KAN Train Loss: 0.14890198906262717, Validation Loss: 0.16804076358675957\n",
      "Epoch 46, KAN Train Loss: 0.14775361286269295, Validation Loss: 0.1676516979932785\n",
      "Epoch 47, KAN Train Loss: 0.1484263522757424, Validation Loss: 0.16725367680191994\n",
      "Epoch 48, KAN Train Loss: 0.14888993899027506, Validation Loss: 0.1668534129858017\n",
      "Epoch 49, KAN Train Loss: 0.15177071425649855, Validation Loss: 0.16646191477775574\n",
      "Epoch 0, MLP Train Loss: 0.13553250581026077, Validation Loss: 0.13008918426930904\n",
      "Epoch 1, MLP Train Loss: 0.12765396469169193, Validation Loss: 0.12276625633239746\n",
      "Epoch 2, MLP Train Loss: 0.11814663890335295, Validation Loss: 0.11581912450492382\n",
      "Epoch 3, MLP Train Loss: 0.11804842866129345, Validation Loss: 0.10870719328522682\n",
      "Epoch 4, MLP Train Loss: 0.10714235736264123, Validation Loss: 0.10119093954563141\n",
      "Epoch 5, MLP Train Loss: 0.09963806966940562, Validation Loss: 0.09298724494874477\n",
      "Epoch 6, MLP Train Loss: 0.08680155790514416, Validation Loss: 0.0850829016417265\n",
      "Epoch 7, MLP Train Loss: 0.08076559214128388, Validation Loss: 0.07836147584021091\n",
      "Epoch 8, MLP Train Loss: 0.07711416689885987, Validation Loss: 0.07297420874238014\n",
      "Epoch 9, MLP Train Loss: 0.06927540318833457, Validation Loss: 0.06871339771896601\n",
      "Epoch 10, MLP Train Loss: 0.0687597282230854, Validation Loss: 0.06702573411166668\n",
      "Epoch 11, MLP Train Loss: 0.06483997818496492, Validation Loss: 0.0655424389988184\n",
      "Epoch 12, MLP Train Loss: 0.06364326634340817, Validation Loss: 0.06425543781369925\n",
      "Epoch 13, MLP Train Loss: 0.062037378549575806, Validation Loss: 0.06316180992871523\n",
      "Epoch 14, MLP Train Loss: 0.06275391040576829, Validation Loss: 0.062222400680184364\n",
      "Epoch 15, MLP Train Loss: 0.06198660615417692, Validation Loss: 0.06135443039238453\n",
      "Epoch 16, MLP Train Loss: 0.06007604094015227, Validation Loss: 0.06062343064695597\n",
      "Epoch 17, MLP Train Loss: 0.058237222126788564, Validation Loss: 0.05993947386741638\n",
      "Epoch 18, MLP Train Loss: 0.060070699287785426, Validation Loss: 0.059375884011387825\n",
      "Epoch 19, MLP Train Loss: 0.05877473081151644, Validation Loss: 0.058958299458026886\n",
      "Epoch 20, MLP Train Loss: 0.058198518223232694, Validation Loss: 0.058744936250150204\n",
      "Epoch 21, MLP Train Loss: 0.05948693346646097, Validation Loss: 0.05857289396226406\n",
      "Epoch 22, MLP Train Loss: 0.06058925804164675, Validation Loss: 0.05840493459254503\n",
      "Epoch 23, MLP Train Loss: 0.06094919186499384, Validation Loss: 0.058222305960953236\n",
      "Epoch 24, MLP Train Loss: 0.05770545577009519, Validation Loss: 0.058093211613595486\n",
      "Epoch 25, MLP Train Loss: 0.060070770068301096, Validation Loss: 0.05796181969344616\n",
      "Epoch 26, MLP Train Loss: 0.06054246177275976, Validation Loss: 0.057882572524249554\n",
      "Epoch 27, MLP Train Loss: 0.05635282231701745, Validation Loss: 0.05778627656400204\n",
      "Epoch 28, MLP Train Loss: 0.05909675980607668, Validation Loss: 0.0576715599745512\n",
      "Epoch 29, MLP Train Loss: 0.06036839716964298, Validation Loss: 0.0575371403247118\n",
      "Epoch 30, MLP Train Loss: 0.060158762252993055, Validation Loss: 0.05749675910919905\n",
      "Epoch 31, MLP Train Loss: 0.05853662515679995, Validation Loss: 0.05745197646319866\n",
      "Epoch 32, MLP Train Loss: 0.058505369557274714, Validation Loss: 0.05740693211555481\n",
      "Epoch 33, MLP Train Loss: 0.05823023493091265, Validation Loss: 0.057380990125238895\n",
      "Epoch 34, MLP Train Loss: 0.05953213572502136, Validation Loss: 0.057338811457157135\n",
      "Epoch 35, MLP Train Loss: 0.05996942395965258, Validation Loss: 0.05730939004570246\n",
      "Epoch 36, MLP Train Loss: 0.05890546076827579, Validation Loss: 0.05727727152407169\n",
      "Epoch 37, MLP Train Loss: 0.05746859063704809, Validation Loss: 0.05724144075065851\n",
      "Epoch 38, MLP Train Loss: 0.05706131334106127, Validation Loss: 0.05720731243491173\n",
      "Epoch 39, MLP Train Loss: 0.05744181035293473, Validation Loss: 0.05718374811112881\n",
      "Epoch 40, MLP Train Loss: 0.057661313149664134, Validation Loss: 0.05717113986611366\n",
      "Epoch 41, MLP Train Loss: 0.05875944594542185, Validation Loss: 0.05716163478791714\n",
      "Epoch 42, MLP Train Loss: 0.05738740124636226, Validation Loss: 0.057142240926623344\n",
      "Epoch 43, MLP Train Loss: 0.05649610153502888, Validation Loss: 0.05713458266109228\n",
      "Epoch 44, MLP Train Loss: 0.058578496591912374, Validation Loss: 0.05711555015295744\n",
      "Epoch 45, MLP Train Loss: 0.06129421624872419, Validation Loss: 0.05710569862276316\n",
      "Epoch 46, MLP Train Loss: 0.060332592162821025, Validation Loss: 0.057092368602752686\n",
      "Epoch 47, MLP Train Loss: 0.06010622365607156, Validation Loss: 0.05707028601318598\n",
      "Epoch 48, MLP Train Loss: 0.05770311587386661, Validation Loss: 0.05706626642495394\n",
      "Epoch 49, MLP Train Loss: 0.059417451835340925, Validation Loss: 0.057065052911639214\n",
      "Epoch 0, KAC_Net Train Loss: 0.4285686214764913, Validation Loss: 0.3696408048272133\n",
      "Epoch 1, KAC_Net Train Loss: 0.401620798640781, Validation Loss: 0.36064406484365463\n",
      "Epoch 2, KAC_Net Train Loss: 0.39982495374149746, Validation Loss: 0.3540569171309471\n",
      "Epoch 3, KAC_Net Train Loss: 0.39704886741108364, Validation Loss: 0.34874092787504196\n",
      "Epoch 4, KAC_Net Train Loss: 0.38830211758613586, Validation Loss: 0.34214985370635986\n",
      "Epoch 5, KAC_Net Train Loss: 0.3884167406294081, Validation Loss: 0.335752435028553\n",
      "Epoch 6, KAC_Net Train Loss: 0.3781568706035614, Validation Loss: 0.3296414613723755\n",
      "Epoch 7, KAC_Net Train Loss: 0.3716150124867757, Validation Loss: 0.32376778870821\n",
      "Epoch 8, KAC_Net Train Loss: 0.36468081341849434, Validation Loss: 0.3181225433945656\n",
      "Epoch 9, KAC_Net Train Loss: 0.358327673541175, Validation Loss: 0.31248465925455093\n",
      "Epoch 10, KAC_Net Train Loss: 0.355365965101454, Validation Loss: 0.3097177669405937\n",
      "Epoch 11, KAC_Net Train Loss: 0.3504904508590698, Validation Loss: 0.30700819194316864\n",
      "Epoch 12, KAC_Net Train Loss: 0.344244572851393, Validation Loss: 0.3043307811021805\n",
      "Epoch 13, KAC_Net Train Loss: 0.3434459865093231, Validation Loss: 0.30168459564447403\n",
      "Epoch 14, KAC_Net Train Loss: 0.3375871578852336, Validation Loss: 0.29906240850687027\n",
      "Epoch 15, KAC_Net Train Loss: 0.3358479373984867, Validation Loss: 0.2964640259742737\n",
      "Epoch 16, KAC_Net Train Loss: 0.33052439159817165, Validation Loss: 0.29393596947193146\n",
      "Epoch 17, KAC_Net Train Loss: 0.33136866490046185, Validation Loss: 0.2914093807339668\n",
      "Epoch 18, KAC_Net Train Loss: 0.3276406526565552, Validation Loss: 0.28888148069381714\n",
      "Epoch 19, KAC_Net Train Loss: 0.3266323341263665, Validation Loss: 0.2864169329404831\n",
      "Epoch 20, KAC_Net Train Loss: 0.3245388865470886, Validation Loss: 0.2851700857281685\n",
      "Epoch 21, KAC_Net Train Loss: 0.3263181083732181, Validation Loss: 0.2839241251349449\n",
      "Epoch 22, KAC_Net Train Loss: 0.3170197837882572, Validation Loss: 0.2826766073703766\n",
      "Epoch 23, KAC_Net Train Loss: 0.31570354104042053, Validation Loss: 0.28143857419490814\n",
      "Epoch 24, KAC_Net Train Loss: 0.3198128905561235, Validation Loss: 0.28021352738142014\n",
      "Epoch 25, KAC_Net Train Loss: 0.3129020134607951, Validation Loss: 0.2789725661277771\n",
      "Epoch 26, KAC_Net Train Loss: 0.3170716365178426, Validation Loss: 0.2777553200721741\n",
      "Epoch 27, KAC_Net Train Loss: 0.30960196918911403, Validation Loss: 0.27653080970048904\n",
      "Epoch 28, KAC_Net Train Loss: 0.31009528372022843, Validation Loss: 0.27531950920820236\n",
      "Epoch 29, KAC_Net Train Loss: 0.30865799718432957, Validation Loss: 0.2741095796227455\n",
      "Epoch 30, KAC_Net Train Loss: 0.3129934204949273, Validation Loss: 0.27350615710020065\n",
      "Epoch 31, KAC_Net Train Loss: 0.31003108620643616, Validation Loss: 0.27290254831314087\n",
      "Epoch 32, KAC_Net Train Loss: 0.31023901369836593, Validation Loss: 0.27230189740657806\n",
      "Epoch 33, KAC_Net Train Loss: 0.3101559314462874, Validation Loss: 0.2716969773173332\n",
      "Epoch 34, KAC_Net Train Loss: 0.31110014849238926, Validation Loss: 0.2711018696427345\n",
      "Epoch 35, KAC_Net Train Loss: 0.3095902270740933, Validation Loss: 0.27051232755184174\n",
      "Epoch 36, KAC_Net Train Loss: 0.309037403927909, Validation Loss: 0.2699126750230789\n",
      "Epoch 37, KAC_Net Train Loss: 0.3049212528599633, Validation Loss: 0.26931845396757126\n",
      "Epoch 38, KAC_Net Train Loss: 0.3109407474597295, Validation Loss: 0.2687279172241688\n",
      "Epoch 39, KAC_Net Train Loss: 0.3036763932969835, Validation Loss: 0.26813704520463943\n",
      "Epoch 40, KAC_Net Train Loss: 0.3045733918746312, Validation Loss: 0.2678416967391968\n",
      "Epoch 41, KAC_Net Train Loss: 0.3070162302917904, Validation Loss: 0.267544474452734\n",
      "Epoch 42, KAC_Net Train Loss: 0.3014453881316715, Validation Loss: 0.2672474756836891\n",
      "Epoch 43, KAC_Net Train Loss: 0.3055564992957645, Validation Loss: 0.2669546604156494\n",
      "Epoch 44, KAC_Net Train Loss: 0.3084641959932115, Validation Loss: 0.2666620574891567\n",
      "Epoch 45, KAC_Net Train Loss: 0.300019277466668, Validation Loss: 0.26637061685323715\n",
      "Epoch 46, KAC_Net Train Loss: 0.30386323233445484, Validation Loss: 0.2660767175257206\n",
      "Epoch 47, KAC_Net Train Loss: 0.2989201661613252, Validation Loss: 0.2657841481268406\n",
      "Epoch 48, KAC_Net Train Loss: 0.3064361545774672, Validation Loss: 0.26548919826745987\n",
      "Epoch 49, KAC_Net Train Loss: 0.2991667124960158, Validation Loss: 0.2651966065168381\n",
      "Epoch 0, KAL_Net Train Loss: 0.2889859014087253, Validation Loss: 0.29159560799598694\n",
      "Epoch 1, KAL_Net Train Loss: 0.27362540033128524, Validation Loss: 0.27603453397750854\n",
      "Epoch 2, KAL_Net Train Loss: 0.2559693422582414, Validation Loss: 0.2694202959537506\n",
      "Epoch 3, KAL_Net Train Loss: 0.24572911030716366, Validation Loss: 0.2650134898722172\n",
      "Epoch 4, KAL_Net Train Loss: 0.24397588272889456, Validation Loss: 0.2596459910273552\n",
      "Epoch 5, KAL_Net Train Loss: 0.23455742001533508, Validation Loss: 0.2582504004240036\n",
      "Epoch 6, KAL_Net Train Loss: 0.23721787167920005, Validation Loss: 0.2545934244990349\n",
      "Epoch 7, KAL_Net Train Loss: 0.22579587664869097, Validation Loss: 0.2485281527042389\n",
      "Epoch 8, KAL_Net Train Loss: 0.23159874313407475, Validation Loss: 0.2457399219274521\n",
      "Epoch 9, KAL_Net Train Loss: 0.22644215491082934, Validation Loss: 0.24023526534438133\n",
      "Epoch 10, KAL_Net Train Loss: 0.209176199303733, Validation Loss: 0.23646286502480507\n",
      "Epoch 11, KAL_Net Train Loss: 0.21727335784170362, Validation Loss: 0.23492631688714027\n",
      "Epoch 12, KAL_Net Train Loss: 0.20925817555851406, Validation Loss: 0.23375333473086357\n",
      "Epoch 13, KAL_Net Train Loss: 0.20806561907132468, Validation Loss: 0.22902534157037735\n",
      "Epoch 14, KAL_Net Train Loss: 0.20693031781249577, Validation Loss: 0.22802216559648514\n",
      "Epoch 15, KAL_Net Train Loss: 0.20924426118532816, Validation Loss: 0.22685116901993752\n",
      "Epoch 16, KAL_Net Train Loss: 0.20392781992753348, Validation Loss: 0.22569718584418297\n",
      "Epoch 17, KAL_Net Train Loss: 0.2049582021103965, Validation Loss: 0.22559866309165955\n",
      "Epoch 18, KAL_Net Train Loss: 0.20108448134528267, Validation Loss: 0.22578713297843933\n",
      "Epoch 19, KAL_Net Train Loss: 0.20536806186040243, Validation Loss: 0.2250349149107933\n",
      "Epoch 20, KAL_Net Train Loss: 0.2030816160970264, Validation Loss: 0.2233951911330223\n",
      "Epoch 21, KAL_Net Train Loss: 0.20722897185219658, Validation Loss: 0.22012875229120255\n",
      "Epoch 22, KAL_Net Train Loss: 0.20139324996206495, Validation Loss: 0.21964658796787262\n",
      "Epoch 23, KAL_Net Train Loss: 0.19983833531538645, Validation Loss: 0.21910909190773964\n",
      "Epoch 24, KAL_Net Train Loss: 0.19921436740292442, Validation Loss: 0.21853983774781227\n",
      "Epoch 25, KAL_Net Train Loss: 0.19870257874329886, Validation Loss: 0.2179691605269909\n",
      "Epoch 26, KAL_Net Train Loss: 0.20330890847576988, Validation Loss: 0.2174006924033165\n",
      "Epoch 27, KAL_Net Train Loss: 0.19847715728812748, Validation Loss: 0.21684952080249786\n",
      "Epoch 28, KAL_Net Train Loss: 0.19308088885413277, Validation Loss: 0.21628006547689438\n",
      "Epoch 29, KAL_Net Train Loss: 0.1983724964989556, Validation Loss: 0.21571746841073036\n",
      "Epoch 30, KAL_Net Train Loss: 0.20000981787840524, Validation Loss: 0.21543274074792862\n",
      "Epoch 31, KAL_Net Train Loss: 0.1942959858311547, Validation Loss: 0.2151692546904087\n",
      "Epoch 32, KAL_Net Train Loss: 0.20126819941732618, Validation Loss: 0.21489543095231056\n",
      "Epoch 33, KAL_Net Train Loss: 0.2000168197684818, Validation Loss: 0.21462277695536613\n",
      "Epoch 34, KAL_Net Train Loss: 0.19313769208060372, Validation Loss: 0.21434296667575836\n",
      "Epoch 35, KAL_Net Train Loss: 0.20071469412909615, Validation Loss: 0.21406839415431023\n",
      "Epoch 36, KAL_Net Train Loss: 0.19381374617417654, Validation Loss: 0.21378466859459877\n",
      "Epoch 37, KAL_Net Train Loss: 0.19742649959193337, Validation Loss: 0.21349337697029114\n",
      "Epoch 38, KAL_Net Train Loss: 0.1904837191104889, Validation Loss: 0.21321934834122658\n",
      "Epoch 39, KAL_Net Train Loss: 0.19525802963309818, Validation Loss: 0.21293367817997932\n",
      "Epoch 40, KAL_Net Train Loss: 0.19260206984149086, Validation Loss: 0.2127927727997303\n",
      "Epoch 41, KAL_Net Train Loss: 0.19145076473553976, Validation Loss: 0.21265611052513123\n",
      "Epoch 42, KAL_Net Train Loss: 0.20109383761882782, Validation Loss: 0.21252217516303062\n",
      "Epoch 43, KAL_Net Train Loss: 0.19383402003182304, Validation Loss: 0.21238325536251068\n",
      "Epoch 44, KAL_Net Train Loss: 0.19507791764206356, Validation Loss: 0.21224000677466393\n",
      "Epoch 45, KAL_Net Train Loss: 0.19875479406780666, Validation Loss: 0.21209801733493805\n",
      "Epoch 46, KAL_Net Train Loss: 0.19423746565977731, Validation Loss: 0.21195290237665176\n",
      "Epoch 47, KAL_Net Train Loss: 0.19200681315528023, Validation Loss: 0.2118157520890236\n",
      "Epoch 48, KAL_Net Train Loss: 0.19364727371268803, Validation Loss: 0.21167705208063126\n",
      "Epoch 49, KAL_Net Train Loss: 0.1921310308906767, Validation Loss: 0.21153847500681877\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m train_and_validate_model(kal_model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, val_loader\u001b[38;5;241m=\u001b[39mval_loader, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKAL_Net\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Evaluate both models\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 导入所需的库\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_model\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 调用evaluate_model函数，传入kan_model、val_loader、\"KAN\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m kan_predictions, kan_actuals \u001b[38;5;241m=\u001b[39m evaluate_model(kan_model, val_loader, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKAN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"kan\")\n",
    "dimension=4\n",
    "# Define model layers\n",
    "layers = [dimension, 9, 5, 2]\n",
    "x_data=torch.cat((input,test_input),0)\n",
    "y_data=torch.cat((ouput,test_label),0)\n",
    "\n",
    "# 定义一个TensorDataset对象，将x_data和y_data传入\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "# 计算训练集和验证集的数量\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# 将数据集划分为训练集和验证集\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# 创建训练集的DataLoader对象，批量大小为32，打乱数据\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# 创建验证集的DataLoader对象，批量大小为32，不打乱数据\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize and train the KAN model\n",
    "kan_model = KAN(layers)\n",
    "train_and_validate_model(kan_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"KAN\")\n",
    "\n",
    "    # Initialize and train the MLP model\n",
    "mlp_model = MLP(layers)\n",
    "train_and_validate_model(mlp_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"MLP\")\n",
    "    # Initialize and train the KAC_net model\n",
    "kac_model = KAC_Net(layers)\n",
    "train_and_validate_model(kac_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"KAC_Net\")\n",
    "    # Initialize and train the KAL_net model\n",
    "kal_model = KAL_Net(layers)\n",
    "train_and_validate_model(kal_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"KAL_Net\")\n",
    "    # Evaluate both models\n",
    "# 导入所需的库\n",
    "from utils import evaluate_model\n",
    "# 调用evaluate_model函数，传入kan_model、val_loader、\"KAN\"\n",
    "kan_predictions, kan_actuals = evaluate_model(kan_model, val_loader, f\"KAN\")\n",
    "# 调用evaluate_model函数，传入mlp_model、val_loader、\"MLP\"\n",
    "mlp_predictions, mlp_actuals = evaluate_model(mlp_model, val_loader, f\"MLP\")\n",
    "# 调用evaluate_model函数，传入kac_model、val_loader、\"KAC_Net\"\n",
    "kac_predictions, kac_actuals = evaluate_model(kac_model, val_loader, f\"KAC_Net\")\n",
    "# 调用evaluate_model函数，传入kal_model、val_loader、\"KAL_Net\"\n",
    "kal_predictions, kal_actuals = evaluate_model(kal_model, val_loader, f\"KAL_Net\")\n",
    "# 定义kan_data，用于存储kan_predictions和kan_actuals\n",
    "    # Log results to wandb\n",
    "# 调用wandb.log函数，传入kan_data\n",
    "kan_data = [[pred, act] for pred, act in zip(kan_predictions, kan_actuals)]\n",
    "# 定义mlp_data，用于存储mlp_predictions和mlp_actuals\n",
    "# 调用wandb.log函数，传入mlp_data\n",
    "mlp_data = [[pred, act] for pred, act in zip(mlp_predictions, mlp_actuals)]\n",
    "# 定义kac_data，用于存储kac_predictions和kac_actuals\n",
    "# 调用wandb.log函数，传入kac_data\n",
    "kac_data = [[pred, act] for pred, act in zip(kac_predictions, kac_actuals)]\n",
    "# 定义kal_data，用于存储kal_predictions和kal_actuals\n",
    "# 调用wandb.log函数，传入kal_data\n",
    "kal_data = [[pred, act] for pred, act in zip(kal_predictions, kal_actuals)]\n",
    "wandb.log({\n",
    "        f\"KAN Predictions vs Actuals\": wandb.Table(data=kan_data, columns=[\"KAN Predictions\", \"Actuals\"]),\n",
    "        f\"MLP Predictions vs Actuals\": wandb.Table(data=mlp_data, columns=[\"MLP Predictions\", \"Actuals\"]),\n",
    "        f\"KAC_Net Predictions vs Actuals\": wandb.Table(data=kac_data, columns=[\"KAC_Net Predictions\", \"Actuals\"]),\n",
    "        f\"KAL_Net Predictions vs Actuals\": wandb.Table(data=kal_data, columns=[\"KAL_Net Predictions\", \"Actuals\"]),\n",
    "    \n",
    "    })\n",
    "\n",
    "    # Save model states\n",
    "# 保存kan_model的状态字典到文件\"kan inverse.pth\"\n",
    "torch.save(kan_model.state_dict(), f\"kan inverse.pth\")\n",
    "# 保存mlp_model的状态字典到文件\"mlp inverse.pth\"\n",
    "torch.save(mlp_model.state_dict(), f\"mlp inverse.pth\")\n",
    "# 保存kac_model的状态字典到文件\"kac_net inverse.pth\"\n",
    "torch.save(kac_model.state_dict(), f\"kac_net inverse.pth\")\n",
    "# 保存kal_model的状态字典到文件\"kal_net inverse.pth\"\n",
    "torch.save(kal_model.state_dict(), f\"kal_net inverse.pth\")\n",
    "# 保存\"kan inverse.pth\"文件到wandb\n",
    "wandb.save(f\"kan inverse.pth\")\n",
    "# 保存\"mlp inverse.pth\"文件到wandb\n",
    "wandb.save(f\"mlp inverse.pth\")\n",
    "# 保存\"kac_net inverse.pth\"文件到wandb\n",
    "wandb.save(f\"kac_net inverse.pth\")\n",
    "# 保存\"kal_net inverse.pth\"文件到wandb\n",
    "wandb.save(f\"kal_net inverse.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_predictions_to_wandb(model_name, predictions, actuals):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(predictions, label='Predictions')\n",
    "    plt.plot(actuals, label='Actuals')\n",
    "    plt.title(f'{model_name} Predictions vs Actuals')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 保存图表到文件\n",
    "    plt.savefig(f\"{model_name}_predictions_vs_actuals.png\")\n",
    "    \n",
    "    # 使用W&B记录图表\n",
    "    wandb.log({f\"{model_name}_predictions_vs_actuals\": wandb.Image(f\"{model_name}_predictions_vs_actuals.png\")})\n",
    "    \n",
    "    # 清除当前的图表，以便下一次绘图\n",
    "    plt.clf()\n",
    "\n",
    "# 记录KAN模型的结果\n",
    "log_predictions_to_wandb(\"KAN\", kan_predictions, kan_actuals)\n",
    "\n",
    "# 记录MLP模型的结果\n",
    "log_predictions_to_wandb(\"MLP\", mlp_predictions, mlp_actuals)\n",
    "\n",
    "# 记录KAC_Net模型的结果\n",
    "log_predictions_to_wandb(\"KAC_Net\", kac_predictions, kac_actuals)\n",
    "\n",
    "# 记录KAL_Net模型的结果\n",
    "log_predictions_to_wandb(\"KAL_Net\", kal_predictions, kal_actuals)\n",
    "    # Save model states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
