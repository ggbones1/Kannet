{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "from torchkan import KAN\n",
    "from KACnet import KAC_Net\n",
    "from KALnet import KAL_Net\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, epochs, learning_rate, train_loader, val_loader, model_name):\n",
    "    '''\n",
    "    这段代码是用Python编写的，主要用于训练一个神经网络模型。以下是代码的详细解释：\n",
    "\n",
    "1. 首先，导入所需的库和模块。这里使用了PyTorch（nn和optim模块）和wandb（用于跟踪训练过程）。\n",
    "\n",
    "2. 定义损失函数（均方误差损失）和优化器（随机梯度下降）。\n",
    "\n",
    "3. 定义学习率调度器（StepLR），用于在每个训练周期后调整学习率。这里设置了步长为10，gamma为0.5，这意味着每10个周期，学习率将乘以0.5。\n",
    "\n",
    "4. 定义训练和验证数据加载器（train_loader和val_loader）。\n",
    "\n",
    "5. 定义训练循环，共进行epochs个周期。在每个周期中：\n",
    "\n",
    "   a. 将模型设置为训练模式（model.train()）。\n",
    "\n",
    "   b. 初始化总损失为0（total_loss = 0）。\n",
    "\n",
    "   c. 遍历训练数据加载器（for x, y in train_loader:），对每个输入（x）和目标（y）进行以下操作：\n",
    "\n",
    "      i. 清除梯度（optimizer.zero_grad()）。\n",
    "/\n",
    "      ii. 使用模型预测输出（predicted_y = model(x)）。\n",
    "\n",
    "      iii. 计算损失（loss = loss_fn(predicted_y, y.unsqueeze(1))）。\n",
    "\n",
    "      iv. 反向传播损失（loss.backward()）。\n",
    "\n",
    "      v. 更新模型参数（optimizer.step()）。\n",
    "\n",
    "      vi. 将损失累加到总损失中（total_loss += loss.item()）。\n",
    "\n",
    "   d. 更新学习率（scheduler.step()）。\n",
    "\n",
    "   e. 计算平均训练损失（avg_loss = total_loss / len(train_loader)），并使用wandb记录训练损失（wandb.log({f\"{model_name} Train Loss\": avg_loss})）。\n",
    "\n",
    "6. 将模型设置为评估模式（model.eval()）。\n",
    "\n",
    "7. 初始化总验证损失为0（total_val_loss = 0）。\n",
    "\n",
    "8. 遍历验证数据加载器（for x, y in val_loader:），对每个输入（x）和目标（y）进行以下操作：\n",
    "\n",
    "   a. 使用模型预测输出（predicted_y = model(x)）。\n",
    "\n",
    "   b. 计算验证损失（val_loss = loss_fn(predicted_y, y.unsqueeze(1))）。\n",
    "\n",
    "   c. 将验证损失累加到总验证损失中（total_val_loss += val_loss.item()）。\n",
    "\n",
    "9. 计算平均验证损失（avg_val_loss = total_val_loss / len(val_loader)），并使用wandb记录验证损失（wandb.log({f\"{model_name} Validation Loss\": avg_val_loss})）。\n",
    "\n",
    "10. 打印训练和验证损失（print(f\"Epoch {epoch}, {model_name} Train Loss: {avg_loss}, Validation Loss: {avg_val_loss}\")）。\n",
    "\n",
    "总之，这段代码实现了一个简单的神经网络模型训练过程，包括训练和验证损失的计算、学习率的调度以及训练过程的记录。\n",
    "\n",
    "'''\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predicted_y = model(x)\n",
    "            loss = loss_fn(predicted_y, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                predicted_y = model(x)\n",
    "                val_loss = loss_fn(predicted_y, y.unsqueeze(1))\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch}, {model_name} Train Loss: {avg_loss}, Validation Loss: {avg_val_loss}\")\n",
    "    loss_df = pd.DataFrame({\"Epoch\": list(range(1, epochs + 1)),\n",
    "                        f\"{model_name} Train Loss\": train_losses,\n",
    "                        f\"{model_name} Validation Loss\": val_losses})\n",
    "    loss_df.to_excel(f\"{model_name} loss_result.xlsx\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader, model_name):\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in eval_loader:\n",
    "            predicted_y = model(x)\n",
    "            predictions.extend(predicted_y.squeeze().cpu().numpy())\n",
    "            actuals.extend(y.cpu().numpy())\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义MLP模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        mlp_layers = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            mlp_layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            if i < len(layers) - 2:\n",
    "                mlp_layers.append(nn.ReLU())\n",
    "        self.model = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取并清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数: 307\n",
      "测试集样本数: 77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取xlsx文件\n",
    "file_path = 'D:\\OneDrive - Officials\\OneDrive - Mraz Cindy\\done\\毕设资料\\计算公式说明\\数据库.xlsx'  # 替换为你的xlsx文件路径\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# 打乱行顺序并按8:2的比例分成训练集和测试集\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# 提取第2，3，4，5列的数据\n",
    "input = train_df.iloc[:, 1:5]\n",
    "output = train_df.iloc[:, 5:7]\n",
    "test_input = test_df.iloc[:, 1:5]\n",
    "test_label = test_df.iloc[:, 5:7]\n",
    "# 将DataFrame转换为numpy数组并调整其维度为4\n",
    "array1 = input.to_numpy()\n",
    "array2 = output.to_numpy()\n",
    "array3 = test_input.to_numpy()\n",
    "array4 = test_label.to_numpy()\n",
    "input=torch.tensor(array1, dtype=torch.float32)\n",
    "ouput=torch.tensor(array2, dtype=torch.float32)\n",
    "test_input=torch.tensor(array3, dtype=torch.float32)\n",
    "test_label=torch.tensor(array4, dtype=torch.float32)\n",
    "def normalize_columns(tensor):\n",
    "    # 确保输入是2D张量\n",
    "    assert tensor.dim() == 2, \"Input tensor must be 2D\"\n",
    "    \n",
    "    # 获取最小值和最大值\n",
    "    col_min = tensor.min(dim=0, keepdim=True).values\n",
    "    col_max = tensor.max(dim=0, keepdim=True).values\n",
    "    \n",
    "    # 防止除以零的情况\n",
    "    denom = col_max - col_min\n",
    "    denom[denom == 0] = 1  # 如果列中所有值相等，避免除以零\n",
    "    \n",
    "    # 进行归一化\n",
    "    normalized_tensor = (tensor - col_min) / denom\n",
    "    return normalized_tensor\n",
    "# 对每一列进行归一化\n",
    "input= normalize_columns(input)\n",
    "ouput= normalize_columns(ouput)\n",
    "test_input= normalize_columns(test_input)\n",
    "test_label= normalize_columns(test_label)\n",
    "dataset={'train_input':input,'test_input':test_input,'train_label':ouput,'test_label':test_label}\n",
    "\n",
    "# 输出结果\n",
    "print(\"训练集样本数:\", len(train_df))\n",
    "print(\"测试集样本数:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 跑模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, KAN Train Loss: 0.5403186447090573, Validation Loss: 0.534214086830616\n",
      "Epoch 1, KAN Train Loss: 0.5234737197558085, Validation Loss: 0.5129493325948715\n",
      "Epoch 2, KAN Train Loss: 0.4985547728008694, Validation Loss: 0.4943764954805374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\mycode\\pykan\\kannet\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 2])) that is different to the input size (torch.Size([32, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "f:\\mycode\\pykan\\kannet\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([12, 1, 2])) that is different to the input size (torch.Size([12, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "f:\\mycode\\pykan\\kannet\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([20, 1, 2])) that is different to the input size (torch.Size([20, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, KAN Train Loss: 0.47883936762809753, Validation Loss: 0.46236706525087357\n",
      "Epoch 4, KAN Train Loss: 0.4382740987671746, Validation Loss: 0.42134494334459305\n",
      "Epoch 5, KAN Train Loss: 0.3987474938233693, Validation Loss: 0.39515669643878937\n",
      "Epoch 6, KAN Train Loss: 0.39205041858885026, Validation Loss: 0.38927971571683884\n",
      "Epoch 7, KAN Train Loss: 0.36998121274842155, Validation Loss: 0.3649148941040039\n",
      "Epoch 8, KAN Train Loss: 0.357757310072581, Validation Loss: 0.3406201973557472\n",
      "Epoch 9, KAN Train Loss: 0.31823672519789803, Validation Loss: 0.301581546664238\n",
      "Epoch 10, KAN Train Loss: 0.29306912422180176, Validation Loss: 0.2903292775154114\n",
      "Epoch 11, KAN Train Loss: 0.2990639739566379, Validation Loss: 0.29176606982946396\n",
      "Epoch 12, KAN Train Loss: 0.290588335858451, Validation Loss: 0.2845075875520706\n",
      "Epoch 13, KAN Train Loss: 0.28986629843711853, Validation Loss: 0.2770822048187256\n",
      "Epoch 14, KAN Train Loss: 0.2818249397807651, Validation Loss: 0.2713356390595436\n",
      "Epoch 15, KAN Train Loss: 0.2734884950849745, Validation Loss: 0.26691248267889023\n",
      "Epoch 16, KAN Train Loss: 0.26771947907076943, Validation Loss: 0.26282574981451035\n",
      "Epoch 17, KAN Train Loss: 0.26423952480157215, Validation Loss: 0.2536918446421623\n",
      "Epoch 18, KAN Train Loss: 0.26174372765753007, Validation Loss: 0.2472982034087181\n",
      "Epoch 19, KAN Train Loss: 0.2538838055398729, Validation Loss: 0.24336885660886765\n",
      "Epoch 20, KAN Train Loss: 0.24421282609303793, Validation Loss: 0.23820550739765167\n",
      "Epoch 21, KAN Train Loss: 0.2477773278951645, Validation Loss: 0.2361205331981182\n",
      "Epoch 22, KAN Train Loss: 0.24238680965370601, Validation Loss: 0.23170364275574684\n",
      "Epoch 23, KAN Train Loss: 0.2417828275097741, Validation Loss: 0.22982819378376007\n",
      "Epoch 24, KAN Train Loss: 0.23684132761425442, Validation Loss: 0.22767262905836105\n",
      "Epoch 25, KAN Train Loss: 0.23148016134897867, Validation Loss: 0.2216552421450615\n",
      "Epoch 26, KAN Train Loss: 0.22518622875213623, Validation Loss: 0.21564629673957825\n",
      "Epoch 27, KAN Train Loss: 0.22087145845095316, Validation Loss: 0.21280981600284576\n",
      "Epoch 28, KAN Train Loss: 0.2147460381189982, Validation Loss: 0.21010025590658188\n",
      "Epoch 29, KAN Train Loss: 0.21107300453715855, Validation Loss: 0.20889433845877647\n",
      "Epoch 30, KAN Train Loss: 0.21307012107637194, Validation Loss: 0.2080100066959858\n",
      "Epoch 31, KAN Train Loss: 0.2126985357867347, Validation Loss: 0.20681875199079514\n",
      "Epoch 32, KAN Train Loss: 0.20821830133597055, Validation Loss: 0.20551831275224686\n",
      "Epoch 33, KAN Train Loss: 0.2105211830801434, Validation Loss: 0.2037503533065319\n",
      "Epoch 34, KAN Train Loss: 0.20837790767351785, Validation Loss: 0.20161805301904678\n",
      "Epoch 35, KAN Train Loss: 0.20918400916788313, Validation Loss: 0.20063841342926025\n",
      "Epoch 36, KAN Train Loss: 0.20595966610643598, Validation Loss: 0.19959014281630516\n",
      "Epoch 37, KAN Train Loss: 0.21073835425906712, Validation Loss: 0.1972450576722622\n",
      "Epoch 38, KAN Train Loss: 0.1997619867324829, Validation Loss: 0.19572048261761665\n",
      "Epoch 39, KAN Train Loss: 0.2073286771774292, Validation Loss: 0.1946835294365883\n",
      "Epoch 40, KAN Train Loss: 0.1992792867951923, Validation Loss: 0.19416192546486855\n",
      "Epoch 41, KAN Train Loss: 0.19744935466183555, Validation Loss: 0.19365312904119492\n",
      "Epoch 42, KAN Train Loss: 0.2050797012117174, Validation Loss: 0.1931525096297264\n",
      "Epoch 43, KAN Train Loss: 0.1989513701862759, Validation Loss: 0.19264459609985352\n",
      "Epoch 44, KAN Train Loss: 0.20143818193011814, Validation Loss: 0.1921447440981865\n",
      "Epoch 45, KAN Train Loss: 0.20218988756338754, Validation Loss: 0.19162607938051224\n",
      "Epoch 46, KAN Train Loss: 0.1957143396139145, Validation Loss: 0.19110658019781113\n",
      "Epoch 47, KAN Train Loss: 0.19890503750907051, Validation Loss: 0.19059791788458824\n",
      "Epoch 48, KAN Train Loss: 0.19373898373709786, Validation Loss: 0.19009900465607643\n",
      "Epoch 49, KAN Train Loss: 0.19629114700688255, Validation Loss: 0.18959927558898926\n",
      "Epoch 0, MLP Train Loss: 0.4470789366298252, Validation Loss: 0.4273179993033409\n",
      "Epoch 1, MLP Train Loss: 0.42626502447658116, Validation Loss: 0.41513705998659134\n",
      "Epoch 2, MLP Train Loss: 0.4110520945654975, Validation Loss: 0.4036872684955597\n",
      "Epoch 3, MLP Train Loss: 0.405256943570243, Validation Loss: 0.39268267899751663\n",
      "Epoch 4, MLP Train Loss: 0.3938492668999566, Validation Loss: 0.38149942457675934\n",
      "Epoch 5, MLP Train Loss: 0.39277823434935677, Validation Loss: 0.36953287571668625\n",
      "Epoch 6, MLP Train Loss: 0.3671919372346666, Validation Loss: 0.35584234446287155\n",
      "Epoch 7, MLP Train Loss: 0.3545960949526893, Validation Loss: 0.3398091234266758\n",
      "Epoch 8, MLP Train Loss: 0.33552179402775234, Validation Loss: 0.32128968089818954\n",
      "Epoch 9, MLP Train Loss: 0.3205181360244751, Validation Loss: 0.3002115972340107\n",
      "Epoch 10, MLP Train Loss: 0.3096566117472119, Validation Loss: 0.28887832164764404\n",
      "Epoch 11, MLP Train Loss: 0.28859174417124855, Validation Loss: 0.277391642332077\n",
      "Epoch 12, MLP Train Loss: 0.2726958559619056, Validation Loss: 0.26587264239788055\n",
      "Epoch 13, MLP Train Loss: 0.26129893793000114, Validation Loss: 0.2544407993555069\n",
      "Epoch 14, MLP Train Loss: 0.2590218749311235, Validation Loss: 0.24312728643417358\n",
      "Epoch 15, MLP Train Loss: 0.24614236586623722, Validation Loss: 0.23175599426031113\n",
      "Epoch 16, MLP Train Loss: 0.22831548419263628, Validation Loss: 0.22036302089691162\n",
      "Epoch 17, MLP Train Loss: 0.21731929812166426, Validation Loss: 0.2091088742017746\n",
      "Epoch 18, MLP Train Loss: 0.21280552446842194, Validation Loss: 0.1982123777270317\n",
      "Epoch 19, MLP Train Loss: 0.19876432418823242, Validation Loss: 0.18738991767168045\n",
      "Epoch 20, MLP Train Loss: 0.18975680569807687, Validation Loss: 0.1821371503174305\n",
      "Epoch 21, MLP Train Loss: 0.1871665765841802, Validation Loss: 0.17694896459579468\n",
      "Epoch 22, MLP Train Loss: 0.17626475791136423, Validation Loss: 0.17192724347114563\n",
      "Epoch 23, MLP Train Loss: 0.17391437292099, Validation Loss: 0.16707740724086761\n",
      "Epoch 24, MLP Train Loss: 0.16510231792926788, Validation Loss: 0.1623417567461729\n",
      "Epoch 25, MLP Train Loss: 0.15976585116651323, Validation Loss: 0.15773302502930164\n",
      "Epoch 26, MLP Train Loss: 0.1551087275147438, Validation Loss: 0.15330437570810318\n",
      "Epoch 27, MLP Train Loss: 0.1576041297780143, Validation Loss: 0.1489237193018198\n",
      "Epoch 28, MLP Train Loss: 0.15520891381634605, Validation Loss: 0.14471195824444294\n",
      "Epoch 29, MLP Train Loss: 0.14365025444163215, Validation Loss: 0.14053546823561192\n",
      "Epoch 30, MLP Train Loss: 0.1446523012386428, Validation Loss: 0.1385677047073841\n",
      "Epoch 31, MLP Train Loss: 0.13677864356173408, Validation Loss: 0.1366093996912241\n",
      "Epoch 32, MLP Train Loss: 0.13793824199173185, Validation Loss: 0.13472804985940456\n",
      "Epoch 33, MLP Train Loss: 0.1318190958764818, Validation Loss: 0.13281828351318836\n",
      "Epoch 34, MLP Train Loss: 0.13613045546743605, Validation Loss: 0.13100641779601574\n",
      "Epoch 35, MLP Train Loss: 0.12917089213927588, Validation Loss: 0.1291881762444973\n",
      "Epoch 36, MLP Train Loss: 0.12908608300818336, Validation Loss: 0.12744599394500256\n",
      "Epoch 37, MLP Train Loss: 0.12402759409613079, Validation Loss: 0.12569465301930904\n",
      "Epoch 38, MLP Train Loss: 0.12415302462048, Validation Loss: 0.12403657846152782\n",
      "Epoch 39, MLP Train Loss: 0.13105710099140802, Validation Loss: 0.12238855846226215\n",
      "Epoch 40, MLP Train Loss: 0.12696090092261633, Validation Loss: 0.12155958823859692\n",
      "Epoch 41, MLP Train Loss: 0.12392427275578181, Validation Loss: 0.12074593268334866\n",
      "Epoch 42, MLP Train Loss: 0.12177056901984745, Validation Loss: 0.11994044668972492\n",
      "Epoch 43, MLP Train Loss: 0.11683561735683018, Validation Loss: 0.11914457939565182\n",
      "Epoch 44, MLP Train Loss: 0.12702134831084144, Validation Loss: 0.11836559511721134\n",
      "Epoch 45, MLP Train Loss: 0.11668190442853504, Validation Loss: 0.11759238503873348\n",
      "Epoch 46, MLP Train Loss: 0.12052833040555318, Validation Loss: 0.11684016324579716\n",
      "Epoch 47, MLP Train Loss: 0.11485224879450268, Validation Loss: 0.11606517806649208\n",
      "Epoch 48, MLP Train Loss: 0.11762710909048717, Validation Loss: 0.11533026024699211\n",
      "Epoch 49, MLP Train Loss: 0.11769651373227437, Validation Loss: 0.11460045352578163\n",
      "Epoch 0, KAC_Net Train Loss: 0.2641308705012004, Validation Loss: 0.2655947618186474\n",
      "Epoch 1, KAC_Net Train Loss: 0.2604141765170627, Validation Loss: 0.2557307854294777\n",
      "Epoch 2, KAC_Net Train Loss: 0.25236545337571037, Validation Loss: 0.2512427382171154\n",
      "Epoch 3, KAC_Net Train Loss: 0.25048672821786666, Validation Loss: 0.2508322335779667\n",
      "Epoch 4, KAC_Net Train Loss: 0.25182843373881447, Validation Loss: 0.24592042714357376\n",
      "Epoch 5, KAC_Net Train Loss: 0.25285598304536605, Validation Loss: 0.24516189098358154\n",
      "Epoch 6, KAC_Net Train Loss: 0.23411585059430864, Validation Loss: 0.23387548327445984\n",
      "Epoch 7, KAC_Net Train Loss: 0.22748331725597382, Validation Loss: 0.2306123450398445\n",
      "Epoch 8, KAC_Net Train Loss: 0.22511739863289726, Validation Loss: 0.22740675136446953\n",
      "Epoch 9, KAC_Net Train Loss: 0.22069481511910757, Validation Loss: 0.22173725441098213\n",
      "Epoch 10, KAC_Net Train Loss: 0.2194342960913976, Validation Loss: 0.2202799655497074\n",
      "Epoch 11, KAC_Net Train Loss: 0.21423027416070303, Validation Loss: 0.2188597358763218\n",
      "Epoch 12, KAC_Net Train Loss: 0.2091669887304306, Validation Loss: 0.21549563854932785\n",
      "Epoch 13, KAC_Net Train Loss: 0.21259239150418174, Validation Loss: 0.21412909030914307\n",
      "Epoch 14, KAC_Net Train Loss: 0.21395514408747354, Validation Loss: 0.2127399854362011\n",
      "Epoch 15, KAC_Net Train Loss: 0.2069947106970681, Validation Loss: 0.21135474368929863\n",
      "Epoch 16, KAC_Net Train Loss: 0.207221743133333, Validation Loss: 0.209992665797472\n",
      "Epoch 17, KAC_Net Train Loss: 0.20567168626520368, Validation Loss: 0.2008913867175579\n",
      "Epoch 18, KAC_Net Train Loss: 0.209386451376809, Validation Loss: 0.19947490468621254\n",
      "Epoch 19, KAC_Net Train Loss: 0.19634087052610186, Validation Loss: 0.19804564863443375\n",
      "Epoch 20, KAC_Net Train Loss: 0.20332481463750204, Validation Loss: 0.19733860343694687\n",
      "Epoch 21, KAC_Net Train Loss: 0.19810030029879677, Validation Loss: 0.19663024693727493\n",
      "Epoch 22, KAC_Net Train Loss: 0.20240518119600084, Validation Loss: 0.19593902677297592\n",
      "Epoch 23, KAC_Net Train Loss: 0.1980297863483429, Validation Loss: 0.19525370001792908\n",
      "Epoch 24, KAC_Net Train Loss: 0.2008085681332482, Validation Loss: 0.19457276538014412\n",
      "Epoch 25, KAC_Net Train Loss: 0.2004566192626953, Validation Loss: 0.19388579949736595\n",
      "Epoch 26, KAC_Net Train Loss: 0.19437508947319454, Validation Loss: 0.1932070553302765\n",
      "Epoch 27, KAC_Net Train Loss: 0.19529572625954947, Validation Loss: 0.19251785054802895\n",
      "Epoch 28, KAC_Net Train Loss: 0.19138927261034647, Validation Loss: 0.1918557696044445\n",
      "Epoch 29, KAC_Net Train Loss: 0.1953700151708391, Validation Loss: 0.19117704406380653\n",
      "Epoch 30, KAC_Net Train Loss: 0.19493485324912602, Validation Loss: 0.1908390037715435\n",
      "Epoch 31, KAC_Net Train Loss: 0.1986032939619488, Validation Loss: 0.1905086487531662\n",
      "Epoch 32, KAC_Net Train Loss: 0.1890913943449656, Validation Loss: 0.19016914814710617\n",
      "Epoch 33, KAC_Net Train Loss: 0.19078164961602953, Validation Loss: 0.18983552977442741\n",
      "Epoch 34, KAC_Net Train Loss: 0.19089146455128989, Validation Loss: 0.18950502574443817\n",
      "Epoch 35, KAC_Net Train Loss: 0.1975660655233595, Validation Loss: 0.18917614221572876\n",
      "Epoch 36, KAC_Net Train Loss: 0.1943560093641281, Validation Loss: 0.18884539604187012\n",
      "Epoch 37, KAC_Net Train Loss: 0.19388005468580458, Validation Loss: 0.18851706013083458\n",
      "Epoch 38, KAC_Net Train Loss: 0.1863112880123986, Validation Loss: 0.18818529695272446\n",
      "Epoch 39, KAC_Net Train Loss: 0.19308500488599142, Validation Loss: 0.18785013258457184\n",
      "Epoch 40, KAC_Net Train Loss: 0.18914997743235695, Validation Loss: 0.18768484145402908\n",
      "Epoch 41, KAC_Net Train Loss: 0.1936237547132704, Validation Loss: 0.18752016127109528\n",
      "Epoch 42, KAC_Net Train Loss: 0.18999635345406002, Validation Loss: 0.18735693022608757\n",
      "Epoch 43, KAC_Net Train Loss: 0.1949855105744468, Validation Loss: 0.1871963106095791\n",
      "Epoch 44, KAC_Net Train Loss: 0.19551424351003435, Validation Loss: 0.1870334930717945\n",
      "Epoch 45, KAC_Net Train Loss: 0.19174722333749136, Validation Loss: 0.18686876073479652\n",
      "Epoch 46, KAC_Net Train Loss: 0.18987161914507547, Validation Loss: 0.18670569360256195\n",
      "Epoch 47, KAC_Net Train Loss: 0.20131879217094845, Validation Loss: 0.1865418739616871\n",
      "Epoch 48, KAC_Net Train Loss: 0.19400323265128666, Validation Loss: 0.18637710809707642\n",
      "Epoch 49, KAC_Net Train Loss: 0.18850748075379264, Validation Loss: 0.18621564656496048\n",
      "Epoch 0, KAL_Net Train Loss: 0.28440650966432357, Validation Loss: 0.2725052610039711\n",
      "Epoch 1, KAL_Net Train Loss: 0.25560659501287675, Validation Loss: 0.24960195645689964\n",
      "Epoch 2, KAL_Net Train Loss: 0.24530906975269318, Validation Loss: 0.24051956459879875\n",
      "Epoch 3, KAL_Net Train Loss: 0.24044753279950884, Validation Loss: 0.23384196683764458\n",
      "Epoch 4, KAL_Net Train Loss: 0.23403394056691063, Validation Loss: 0.22989258915185928\n",
      "Epoch 5, KAL_Net Train Loss: 0.23347351948420206, Validation Loss: 0.22624658048152924\n",
      "Epoch 6, KAL_Net Train Loss: 0.2283500764105055, Validation Loss: 0.22289235517382622\n",
      "Epoch 7, KAL_Net Train Loss: 0.22324863407346937, Validation Loss: 0.2197364866733551\n",
      "Epoch 8, KAL_Net Train Loss: 0.2200370447503196, Validation Loss: 0.2167462557554245\n",
      "Epoch 9, KAL_Net Train Loss: 0.21644655697875553, Validation Loss: 0.2139274962246418\n",
      "Epoch 10, KAL_Net Train Loss: 0.21411558157867855, Validation Loss: 0.21258100122213364\n",
      "Epoch 11, KAL_Net Train Loss: 0.21284177899360657, Validation Loss: 0.2112550139427185\n",
      "Epoch 12, KAL_Net Train Loss: 0.21201685236559975, Validation Loss: 0.20994477346539497\n",
      "Epoch 13, KAL_Net Train Loss: 0.21079898873964945, Validation Loss: 0.20871912315487862\n",
      "Epoch 14, KAL_Net Train Loss: 0.21840985284911263, Validation Loss: 0.20742155984044075\n",
      "Epoch 15, KAL_Net Train Loss: 0.20596892138322195, Validation Loss: 0.2061493620276451\n",
      "Epoch 16, KAL_Net Train Loss: 0.20854918824301827, Validation Loss: 0.20487342402338982\n",
      "Epoch 17, KAL_Net Train Loss: 0.21101882225937313, Validation Loss: 0.2036137804389\n",
      "Epoch 18, KAL_Net Train Loss: 0.20533989204300773, Validation Loss: 0.20237111300230026\n",
      "Epoch 19, KAL_Net Train Loss: 0.2102862215704388, Validation Loss: 0.20109020173549652\n",
      "Epoch 20, KAL_Net Train Loss: 0.20035641392072043, Validation Loss: 0.20045996829867363\n",
      "Epoch 21, KAL_Net Train Loss: 0.20200179517269135, Validation Loss: 0.1998528242111206\n",
      "Epoch 22, KAL_Net Train Loss: 0.20017542938391367, Validation Loss: 0.1992221213877201\n",
      "Epoch 23, KAL_Net Train Loss: 0.20284570256868997, Validation Loss: 0.19859661534428596\n",
      "Epoch 24, KAL_Net Train Loss: 0.20280288656552634, Validation Loss: 0.19797268137335777\n",
      "Epoch 25, KAL_Net Train Loss: 0.20239252514309353, Validation Loss: 0.19734860956668854\n",
      "Epoch 26, KAL_Net Train Loss: 0.1990951117542055, Validation Loss: 0.19672949984669685\n",
      "Epoch 27, KAL_Net Train Loss: 0.19960080087184906, Validation Loss: 0.19612328335642815\n",
      "Epoch 28, KAL_Net Train Loss: 0.19680658976236978, Validation Loss: 0.1955147236585617\n",
      "Epoch 29, KAL_Net Train Loss: 0.20000908606582218, Validation Loss: 0.19488992542028427\n",
      "Epoch 30, KAL_Net Train Loss: 0.19908892611662546, Validation Loss: 0.1945800967514515\n",
      "Epoch 31, KAL_Net Train Loss: 0.19869219760100046, Validation Loss: 0.19427156820893288\n",
      "Epoch 32, KAL_Net Train Loss: 0.19670207467344072, Validation Loss: 0.19396039843559265\n",
      "Epoch 33, KAL_Net Train Loss: 0.20070757965246835, Validation Loss: 0.1936563476920128\n",
      "Epoch 34, KAL_Net Train Loss: 0.20340272121959263, Validation Loss: 0.19335509464144707\n",
      "Epoch 35, KAL_Net Train Loss: 0.20031647549735176, Validation Loss: 0.19304602593183517\n",
      "Epoch 36, KAL_Net Train Loss: 0.19561838110287985, Validation Loss: 0.1927361972630024\n",
      "Epoch 37, KAL_Net Train Loss: 0.1950400322675705, Validation Loss: 0.1924256570637226\n",
      "Epoch 38, KAL_Net Train Loss: 0.1898750447564655, Validation Loss: 0.19212501496076584\n",
      "Epoch 39, KAL_Net Train Loss: 0.19524272282918295, Validation Loss: 0.19182364642620087\n",
      "Epoch 40, KAL_Net Train Loss: 0.19422426323095957, Validation Loss: 0.19166873022913933\n",
      "Epoch 41, KAL_Net Train Loss: 0.19165157775084177, Validation Loss: 0.19151493534445763\n",
      "Epoch 42, KAL_Net Train Loss: 0.1964541342523363, Validation Loss: 0.19136208668351173\n",
      "Epoch 43, KAL_Net Train Loss: 0.19653985069857705, Validation Loss: 0.19120966643095016\n",
      "Epoch 44, KAL_Net Train Loss: 0.1958982033862008, Validation Loss: 0.19105757027864456\n",
      "Epoch 45, KAL_Net Train Loss: 0.1924889021449619, Validation Loss: 0.19091006368398666\n",
      "Epoch 46, KAL_Net Train Loss: 0.19947051836384666, Validation Loss: 0.1907607652246952\n",
      "Epoch 47, KAL_Net Train Loss: 0.19753214220205942, Validation Loss: 0.19061227142810822\n",
      "Epoch 48, KAL_Net Train Loss: 0.1957439531882604, Validation Loss: 0.1904592514038086\n",
      "Epoch 49, KAL_Net Train Loss: 0.19471347828706106, Validation Loss: 0.19030863791704178\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dimension=4\n",
    "# Define model layers\n",
    "layers = [dimension, 9, 5, 2]\n",
    "x_data=torch.cat((input,test_input),0)\n",
    "y_data=torch.cat((ouput,test_label),0)\n",
    "\n",
    "# 定义一个TensorDataset对象，将x_data和y_data传入\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "# 计算训练集和验证集的数量\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# 将数据集划分为训练集和验证集\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# 创建训练集的DataLoader对象，批量大小为32，打乱数据\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# 创建验证集的DataLoader对象，批量大小为32，不打乱数据\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize and train the KAN model\n",
    "kan_model = KAN(layers)\n",
    "train_and_validate_model(kan_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"KAN\")\n",
    "    # Initialize and train the MLP model\n",
    "mlp_model = MLP(layers)\n",
    "train_and_validate_model(mlp_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"MLP\")\n",
    "    # Initialize and train the KAC_net model\n",
    "kac_model = KAC_Net(layers)\n",
    "train_and_validate_model(kac_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"KAC_Net\")\n",
    "    # Initialize and train the KAL_net model\n",
    "kal_model = KAL_Net(layers)\n",
    "train_and_validate_model(kal_model, epochs=50, learning_rate=0.001, train_loader=train_loader, val_loader=val_loader, model_name=f\"KAL_Net\")\n",
    "    # Evaluate both models\n",
    "# 调用evaluate_model函数，传入kan_model、val_loader、\"KAN\"\n",
    "kan_predictions, kan_actuals = evaluate_model(kan_model, val_loader, f\"KAN\")\n",
    "# 调用evaluate_model函数，传入mlp_model、val_loader、\"MLP\"\n",
    "mlp_predictions, mlp_actuals = evaluate_model(mlp_model, val_loader, f\"MLP\")\n",
    "# 调用evaluate_model函数，传入kac_model、val_loader、\"KAC_Net\"\n",
    "kac_predictions, kac_actuals = evaluate_model(kac_model, val_loader, f\"KAC_Net\")\n",
    "# 调用evaluate_model函数，传入kal_model、val_loader、\"KAL_Net\"\n",
    "kal_predictions, kal_actuals = evaluate_model(kal_model, val_loader, f\"KAL_Net\")\n",
    "# 定义kan_data，用于存储kan_predictions和kan_actuals\n",
    "    # Log results to wandb\n",
    "# 调用wandb.log函数，传入kan_data\n",
    "import csv\n",
    "\n",
    "def save_to_csv(kan_predictions, kan_actuals, mlp_predictions, mlp_actuals, kac_predictions, kac_actuals, kal_predictions, kal_actuals):\n",
    "    # 打开或创建 CSV 文件\n",
    "    with open('output.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # 写入标题行\n",
    "        writer.writerow([\"KAN_Predictions\", \"KAN_Actuals\", \"MLP_Predictions\", \"MLP_Actuals\", \"KAC_Net_Predictions\", \"KAC_Net_Actuals\", \"KAL_Net_Predictions\", \"KAL_Net_Actuals\"])\n",
    "\n",
    "        # 写入数据行\n",
    "        for i in range(len(kan_predictions)):\n",
    "            writer.writerow([kan_predictions[i], kan_actuals[i], mlp_predictions[i], mlp_actuals[i], kac_predictions[i], kac_actuals[i], kal_predictions[i], kal_actuals[i]])\n",
    "\n",
    "# 假设您已经有了这些变量的值，调用函数进行保存\n",
    "save_to_csv(kan_predictions, kan_actuals, mlp_predictions, mlp_actuals, kac_predictions, kac_actuals, kal_predictions, kal_actuals)\n",
    "    # Save model states\n",
    "# 保存kan_model的状态字典到文件\"kan inverse.pth\"\n",
    "#torch.save(kan_model.state_dict(), f\"kan inverse.pth\")\n",
    "# 保存mlp_model的状态字典到文件\"mlp inverse.pth\"\n",
    "#torch.save(mlp_model.state_dict(), f\"mlp inverse.pth\")\n",
    "# 保存kac_model的状态字典到文件\"kac_net inverse.pth\"\n",
    "#torch.save(kac_model.state_dict(), f\"kac_net inverse.pth\")\n",
    "# 保存kal_model的状态字典到文件\"kal_net inverse.pth\"\n",
    "#torch.save(kal_model.state_dict(), f\"kal_net inverse.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用shap分析模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining model: KAN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\mycode\\pykan\\kannet\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:243: UserWarning: unrecognized nn.Module: PReLU\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "f:\\mycode\\pykan\\kannet\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:243: UserWarning: unrecognized nn.Module: LayerNorm\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n",
      "f:\\mycode\\pykan\\kannet\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:243: UserWarning: unrecognized nn.Module: GELU\n",
      "  warnings.warn(f'unrecognized nn.Module: {module_type}')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of %f is significant compared to the scale of your model outputs, please post as a github issue, with a reproducible example so we can debug it. Used framework: pytorch - Max. diff: 0.8398767858743668 - Tolerance: 0.01",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplaining model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     shap_values, all_data \u001b[38;5;241m=\u001b[39m \u001b[43mexplain_model_with_shap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Plot the SHAP values\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values, all_data, feature_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature 1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature 2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature 3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature 4\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[26], line 11\u001b[0m, in \u001b[0;36mexplain_model_with_shap\u001b[1;34m(model, data_loader)\u001b[0m\n\u001b[0;32m      8\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model, all_data)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Get SHAP values\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m shap_values, all_data\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:159\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:214\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    212\u001b[0m             model_output_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39mX)\n\u001b[1;32m--> 214\u001b[0m     \u001b[43m_check_additivity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_phis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_phis, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# in this case we have multiple inputs and potentially multiple outputs\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_phis[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_utils.py:20\u001b[0m, in \u001b[0;36m_check_additivity\u001b[1;34m(explainer, model_output_values, output_phis)\u001b[0m\n\u001b[0;32m     16\u001b[0m         diffs \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m output_phis[t][i]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, output_phis[t][i]\u001b[38;5;241m.\u001b[39mndim)))\n\u001b[0;32m     18\u001b[0m maxdiff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(diffs)\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m maxdiff \u001b[38;5;241m<\u001b[39m TOLERANCE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe SHAP explanations do not sum up to the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output! This is either because of a \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m     21\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrounding error or because an operator in your computation graph was not fully supported. If \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m     22\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe sum difference of \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m is significant compared to the scale of your model outputs, please post \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m     23\u001b[0m                             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas a github issue, with a reproducible example so we can debug it. Used framework: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplainer\u001b[38;5;241m.\u001b[39mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Max. diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaxdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Tolerance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOLERANCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: The SHAP explanations do not sum up to the model's output! This is either because of a rounding error or because an operator in your computation graph was not fully supported. If the sum difference of %f is significant compared to the scale of your model outputs, please post as a github issue, with a reproducible example so we can debug it. Used framework: pytorch - Max. diff: 0.8398767858743668 - Tolerance: 0.01"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "def explain_model_with_shap(model, data_loader):\n",
    "    # Convert DataLoader to a single tensor for SHAP\n",
    "    all_data = torch.cat([data for data, _ in data_loader], dim=0)\n",
    "    \n",
    "    # Create a SHAP explainer\n",
    "    explainer = shap.DeepExplainer(model, all_data)\n",
    "    \n",
    "    # Get SHAP values\n",
    "    shap_values = explainer.shap_values(all_data)\n",
    "    \n",
    "    return shap_values, all_data\n",
    "# After training the models\n",
    "models = {\n",
    "    \"KAN\": kan_model,\n",
    "    \"MLP\": mlp_model,\n",
    "    \"KAC_Net\": kac_model,\n",
    "    \"KAL_Net\": kal_model\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Explaining model: {model_name}\")\n",
    "    shap_values, all_data = explain_model_with_shap(model, val_loader)\n",
    "    \n",
    "    # Plot the SHAP values\n",
    "    shap.summary_plot(shap_values, all_data, feature_names=['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining model: KAN\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m shap_values, all_data \u001b[38;5;241m=\u001b[39m explain_model_with_shap_gradient(model, val_loader)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Plot the SHAP values\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature 1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature 2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature 3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature 4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\site-packages\\shap\\plots\\_beeswarm.py:595\u001b[0m, in \u001b[0;36msummary_legacy\u001b[1;34m(shap_values, features, feature_names, max_display, plot_type, color, axis_color, title, alpha, show, sort, color_bar, plot_size, layered_violin_max_num_bins, class_names, class_inds, color_bar_label, cmap, show_values_in_legend, use_log_scale)\u001b[0m\n\u001b[0;32m    591\u001b[0m proj_shap_values \u001b[38;5;241m=\u001b[39m shap_values[:, sort_inds[\u001b[38;5;241m0\u001b[39m], sort_inds]\n\u001b[0;32m    592\u001b[0m proj_shap_values[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# because off diag effects are split in half\u001b[39;00m\n\u001b[0;32m    593\u001b[0m summary_legacy(\n\u001b[0;32m    594\u001b[0m     proj_shap_values, features[:, sort_inds] \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m--> 595\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39m\u001b[43mfeature_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msort_inds\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m    596\u001b[0m     sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, color_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m     plot_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m     max_display\u001b[38;5;241m=\u001b[39mmax_display\n\u001b[0;32m    599\u001b[0m )\n\u001b[0;32m    600\u001b[0m pl\u001b[38;5;241m.\u001b[39mxlim((slow, shigh))\n\u001b[0;32m    601\u001b[0m pl\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1150x660 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shap\n",
    "import torch\n",
    "\n",
    "def explain_model_with_shap_gradient(model, data_loader):\n",
    "    model.eval()\n",
    "    all_data = torch.cat([data for data, _ in data_loader], dim=0)\n",
    "    \n",
    "    explainer = shap.GradientExplainer(model, all_data)\n",
    "    shap_values = explainer.shap_values(all_data)\n",
    "    \n",
    "    return shap_values, all_data\n",
    "# After training the models\n",
    "models = {\n",
    "    \"KAN\": kan_model,\n",
    "    \"MLP\": mlp_model,\n",
    "    \"KAC_Net\": kac_model,\n",
    "    \"KAL_Net\": kal_model\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Explaining model: {model_name}\")\n",
    "    shap_values, all_data = explain_model_with_shap_gradient(model, val_loader)\n",
    "    \n",
    "    # Plot the SHAP values\n",
    "    shap.summary_plot(shap_values, all_data, feature_names=['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 116 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining model: KAN\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc1fe388d174cb3bfacf95833f17d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m shap_values, all_data \u001b[38;5;241m=\u001b[39m explain_model_with_shap_kernel(model, val_loader)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Plot the SHAP values\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature 1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature 2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature 3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature 4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\mycode\\pykan\\kannet\\Lib\\site-packages\\shap\\plots\\_beeswarm.py:595\u001b[0m, in \u001b[0;36msummary_legacy\u001b[1;34m(shap_values, features, feature_names, max_display, plot_type, color, axis_color, title, alpha, show, sort, color_bar, plot_size, layered_violin_max_num_bins, class_names, class_inds, color_bar_label, cmap, show_values_in_legend, use_log_scale)\u001b[0m\n\u001b[0;32m    591\u001b[0m proj_shap_values \u001b[38;5;241m=\u001b[39m shap_values[:, sort_inds[\u001b[38;5;241m0\u001b[39m], sort_inds]\n\u001b[0;32m    592\u001b[0m proj_shap_values[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# because off diag effects are split in half\u001b[39;00m\n\u001b[0;32m    593\u001b[0m summary_legacy(\n\u001b[0;32m    594\u001b[0m     proj_shap_values, features[:, sort_inds] \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m--> 595\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39m\u001b[43mfeature_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msort_inds\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m    596\u001b[0m     sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, color_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m     plot_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m     max_display\u001b[38;5;241m=\u001b[39mmax_display\n\u001b[0;32m    599\u001b[0m )\n\u001b[0;32m    600\u001b[0m pl\u001b[38;5;241m.\u001b[39mxlim((slow, shigh))\n\u001b[0;32m    601\u001b[0m pl\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKsAAAItCAYAAABGnor1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWMElEQVR4nO3af2xV9f3H8VdbuLeYcG91HbctKzRo/DF/0FnT5uIMcblbEw0bf4luqY1ROpMuGTSbtANpOpyXGGdISB0bGXTJzIoj4pbRlLkbiJmrISk0QSositrOeC90C+ciSjtvP98/Fu++XVvoqb0tb/p8JOeP++Fz7vmcnCeXew8nzznnBBiQP9cLAKaKWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhhu9YX3/9da1Zs0ZlZWXKy8vTq6++esV9jhw5orvvvlvBYFA33XSTOjo6prFUzHe+Y7148aJWrlyp9vb2Kc1/77339OCDD+r+++9XX1+fNmzYoCeeeEKHDh3yvVjMb3lf5EGWvLw8HThwQGvXrp10zqZNm3Tw4EG99dZb2bGHH35Y58+fV3d393QPjXloQa4P0NPTo1gsNmastrZWGzZsmHSf4eFhDQ8PZ1+Pjo7qX//6l770pS8pLy8vV0vFDHHO6cKFCyorK1N+/sz9LMp5rMlkUpFIZMxYJBJROp3Wp59+qkWLFo3bJx6Pq62tLddLQ44NDg7qK1/5yoy9X85jnY6WlhY1NTVlX3uep2XLlmlwcFChUGgOV4apSKfTKi8v1+LFi2f0fXMea0lJiVKp1JixVCqlUCg04aeqJAWDQQWDwXHjoVCIWA2Z6a9sOb/PGo1GlUgkxoy99tprikajuT40rjG+Y/3444/V19envr4+Sf+5NdXX16eBgQFJ//kn/NFHH83Of/LJJ3XmzBk99dRTOnXqlF588UW9/PLL2rhx48ycAeYP59Phw4edpHFbfX29c865+vp6t3r16nH7VFZWukAg4FasWOH27t3r65ie5zlJzvM8v8vFHMjV9fpC91lnSzqdVjgclud5fGc1IFfXi2cDYAaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGdOKtb29XRUVFSosLFRNTY2OHj162fk7duzQLbfcokWLFqm8vFwbN27UpUuXprVgzGPOp87OThcIBNyePXvcyZMn3fr1611RUZFLpVITzn/ppZdcMBh0L730knvvvffcoUOHXGlpqdu4ceOUj+l5npPkPM/zu1zMgVxdL9+xVldXu8bGxuzrTCbjysrKXDwen3B+Y2Oj+8Y3vjFmrKmpyd17771TPiax2pKr6+Xra8DIyIh6e3sVi8WyY/n5+YrFYurp6Zlwn1WrVqm3tzf7VeHMmTPq6urSAw88MOlxhoeHlU6nx2zAAj+Th4aGlMlkFIlExoxHIhGdOnVqwn2++93vamhoSF//+tflnNNnn32mJ598Uj/5yU8mPU48HldbW5ufpWEeyPndgCNHjujZZ5/Viy++qGPHjumVV17RwYMHtW3btkn3aWlpked52W1wcDDXy4QBvj5Zi4uLVVBQoFQqNWY8lUqppKRkwn2efvpp1dXV6YknnpAk3Xnnnbp48aIaGhq0efNm5eeP//sSDAYVDAb9LA3zgK9P1kAgoKqqKiUSiezY6OioEomEotHohPt88skn44IsKCiQJDnn/K4X85nfX2SdnZ0uGAy6jo4O19/f7xoaGlxRUZFLJpPOOefq6upcc3Nzdn5ra6tbvHix+93vfufOnDnj/vznP7sbb7zRPfTQQ1M+JncDbMnV9fL1NUCS1q1bp3Pnzmnr1q1KJpOqrKxUd3d39kfXwMDAmE/SLVu2KC8vT1u2bNGHH36oL3/5y1qzZo1+9rOfzdTfN8wTec5d/f8Wp9NphcNheZ6nUCg018vBFeTqevFsAMwgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmDGtWNvb21VRUaHCwkLV1NTo6NGjl51//vx5NTY2qrS0VMFgUDfffLO6urqmtWDMXwv87rBv3z41NTVp165dqqmp0Y4dO1RbW6vTp09ryZIl4+aPjIzom9/8ppYsWaL9+/dr6dKl+uCDD1RUVDQT68d84nyqrq52jY2N2deZTMaVlZW5eDw+4fxf/OIXbsWKFW5kZMTvobI8z3OSnOd5034PzJ5cXS9fXwNGRkbU29urWCyWHcvPz1csFlNPT8+E+/zxj39UNBpVY2OjIpGI7rjjDj377LPKZDKTHmd4eFjpdHrMBviKdWhoSJlMRpFIZMx4JBJRMpmccJ8zZ85o//79ymQy6urq0tNPP62f//zneuaZZyY9TjweVzgczm7l5eV+lolrVM7vBoyOjmrJkiX61a9+paqqKq1bt06bN2/Wrl27Jt2npaVFnudlt8HBwVwvEwb4+oFVXFysgoICpVKpMeOpVEolJSUT7lNaWqqFCxeqoKAgO3bbbbcpmUxqZGREgUBg3D7BYFDBYNDP0jAP+PpkDQQCqqqqUiKRyI6Njo4qkUgoGo1OuM+9996rd955R6Ojo9mxv//97yotLZ0wVGBSfn+RdXZ2umAw6Do6Olx/f79raGhwRUVFLplMOuecq6urc83Nzdn5AwMDbvHixe4HP/iBO336tPvTn/7klixZ4p555pkpH5O7Abbk6nr5vs+6bt06nTt3Tlu3blUymVRlZaW6u7uzP7oGBgaUn//fD+zy8nIdOnRIGzdu1F133aWlS5fqhz/8oTZt2jRTf98wT+Q559xcL+JK0um0wuGwPM9TKBSa6+XgCnJ1vXg2AGYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmDGtWNvb21VRUaHCwkLV1NTo6NGjU9qvs7NTeXl5Wrt27XQOi3nOd6z79u1TU1OTWltbdezYMa1cuVK1tbU6e/bsZfd7//339aMf/Uj33XfftBeL+c13rC+88ILWr1+vxx57TF/96le1a9cuXXfdddqzZ8+k+2QyGX3ve99TW1ubVqxY8YUWjPnLV6wjIyPq7e1VLBb77xvk5ysWi6mnp2fS/X76059qyZIlevzxx6d0nOHhYaXT6TEb4CvWoaEhZTIZRSKRMeORSETJZHLCff7617/q17/+tXbv3j3l48TjcYXD4exWXl7uZ5m4RuX0bsCFCxdUV1en3bt3q7i4eMr7tbS0yPO87DY4OJjDVcKKBX4mFxcXq6CgQKlUasx4KpVSSUnJuPnvvvuu3n//fa1ZsyY7Njo6+p8DL1ig06dP68Ybbxy3XzAYVDAY9LM0zAO+PlkDgYCqqqqUSCSyY6Ojo0okEopGo+Pm33rrrTpx4oT6+vqy27e//W3df//96uvr4593+OLrk1WSmpqaVF9fr3vuuUfV1dXasWOHLl68qMcee0yS9Oijj2rp0qWKx+MqLCzUHXfcMWb/oqIiSRo3DlyJ71jXrVunc+fOaevWrUomk6qsrFR3d3f2R9fAwIDy8/mPMcy8POecm+tFXEk6nVY4HJbneQqFQnO9HFxBrq4XH4Ewg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscKMacXa3t6uiooKFRYWqqamRkePHp107u7du3Xffffp+uuv1/XXX69YLHbZ+cBkfMe6b98+NTU1qbW1VceOHdPKlStVW1urs2fPTjj/yJEjeuSRR3T48GH19PSovLxc3/rWt/Thhx9+4cVjnnE+VVdXu8bGxuzrTCbjysrKXDwen9L+n332mVu8eLH7zW9+M+Vjep7nJDnP8/wuF3MgV9fL1yfryMiIent7FYvFsmP5+fmKxWLq6emZ0nt88skn+ve//60bbrhh0jnDw8NKp9NjNsBXrENDQ8pkMopEImPGI5GIksnklN5j06ZNKisrGxP8/4rH4wqHw9mtvLzczzJxjZrVuwHbt29XZ2enDhw4oMLCwknntbS0yPO87DY4ODiLq8TVaoGfycXFxSooKFAqlRoznkqlVFJSctl9n3/+eW3fvl1/+ctfdNddd112bjAYVDAY9LM0zAO+PlkDgYCqqqqUSCSyY6Ojo0okEopGo5Pu99xzz2nbtm3q7u7WPffcM/3VYn7z+4uss7PTBYNB19HR4fr7+11DQ4MrKipyyWTSOedcXV2da25uzs7fvn27CwQCbv/+/e6jjz7KbhcuXJjyMbkbYEuurpfvWJ1zbufOnW7ZsmUuEAi46upq9+abb2b/bPXq1a6+vj77evny5U7SuK21tXXKxyNWW3J1vfKcc27OPtanKJ1OKxwOy/M8hUKhuV4OriBX14tnA2AGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGEGscIMYoUZxAoziBVmECvMIFaYQawwg1hhBrHCDGKFGcQKM4gVZhArzCBWmEGsMINYYQaxwoxpxdre3q6KigoVFhaqpqZGR48evez83//+97r11ltVWFioO++8U11dXdNaLOY337Hu27dPTU1Nam1t1bFjx7Ry5UrV1tbq7NmzE87/29/+pkceeUSPP/64jh8/rrVr12rt2rV66623vvDiMc84n6qrq11jY2P2dSaTcWVlZS4ej084/6GHHnIPPvjgmLGamhr3/e9/f8rH9DzPSXKe5/ldLuZArq7XAj9hj4yMqLe3Vy0tLdmx/Px8xWIx9fT0TLhPT0+PmpqaxozV1tbq1VdfnfQ4w8PDGh4ezr72PE+SlE6n/SwXc+Tz6+Scm9H39RXr0NCQMpmMIpHImPFIJKJTp05NuE8ymZxwfjKZnPQ48XhcbW1t48bLy8v9LBdz7J///KfC4fCMvZ+vWGdLS0vLmE/j8+fPa/ny5RoYGJjRk59r6XRa5eXlGhwcVCgUmuvlzBjP87Rs2TLdcMMNM/q+vmItLi5WQUGBUqnUmPFUKqWSkpIJ9ykpKfE1X5KCwaCCweC48XA4fE1d1M+FQqFr8rzy82f2zqivdwsEAqqqqlIikciOjY6OKpFIKBqNTrhPNBodM1+SXnvttUnnA5Py+4uss7PTBYNB19HR4fr7+11DQ4MrKipyyWTSOedcXV2da25uzs5/44033IIFC9zzzz/v3n77bdfa2uoWLlzoTpw4MeVjXqt3Azgvf3zH6pxzO3fudMuWLXOBQMBVV1e7N998M/tnq1evdvX19WPmv/zyy+7mm292gUDA3X777e7gwYO+jnfp0iXX2trqLl26NJ3lXrU4L3/ynJvh+wtAjvBsAMwgVphBrDCDWGHGVRPrtfrYoZ/z6ujoUF5e3pitsLBwFlc7Na+//rrWrFmjsrIy5eXlXfY5j88dOXJEd999t4LBoG666SZ1dHT4P/CM3luYps7OThcIBNyePXvcyZMn3fr1611RUZFLpVITzn/jjTdcQUGBe+6551x/f7/bsmWL73u3s8Hvee3du9eFQiH30UcfZbfP719fTbq6utzmzZvdK6+84iS5AwcOXHb+mTNn3HXXXeeamppcf3+/27lzpysoKHDd3d2+jntVxDoXjx3OBr/ntXfvXhcOh2dpdTNjKrE+9dRT7vbbbx8ztm7dOldbW+vrWHP+NeDzxw5jsVh2bCqPHf7/+dJ/HjucbP5cmM55SdLHH3+s5cuXq7y8XN/5znd08uTJ2VhuTs3U9ZrzWC/32OFkjxFO57HD2Tad87rlllu0Z88e/eEPf9Bvf/tbjY6OatWqVfrHP/4xG0vOmcmuVzqd1qeffjrl97kqHxGcr6LR6JgHfFatWqXbbrtNv/zlL7Vt27Y5XNnVYc4/WWfrscPZNp3z+l8LFy7U1772Nb3zzju5WOKsmex6hUIhLVq0aMrvM+exXquPHU7nvP5XJpPRiRMnVFpamqtlzooZu15+f/3lwlw8djgb/J5XW1ubO3TokHv33Xddb2+ve/jhh11hYaE7efLkXJ3ChC5cuOCOHz/ujh8/7iS5F154wR0/ftx98MEHzjnnmpubXV1dXXb+57eufvzjH7u3337btbe327115dzsP3Y4W/yc14YNG7JzI5GIe+CBB9yxY8fmYNWXd/jwYSdp3Pb5udTX17vVq1eP26eystIFAgG3YsUKt3fvXt/H5RFBmDHn31mBqSJWmEGsMINYYQaxwgxihRnECjOIFWYQK8wgVphBrDCDWGHG/wH9BtU8LGvejAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1150x660 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shap\n",
    "import torch\n",
    "\n",
    "def explain_model_with_shap_kernel(model, data_loader):\n",
    "    model.eval()\n",
    "    all_data = torch.cat([data for data, _ in data_loader], dim=0)\n",
    "    \n",
    "    def model_predict(x):\n",
    "        with torch.no_grad():\n",
    "            return model(torch.tensor(x, dtype=torch.float)).numpy()\n",
    "    \n",
    "    explainer = shap.KernelExplainer(model_predict, all_data.numpy())\n",
    "    shap_values = explainer.shap_values(all_data.numpy(), nsamples=100)  # Adjust nsamples as needed\n",
    "    \n",
    "    return shap_values, all_data\n",
    "# After training the models\n",
    "models = {\n",
    "    \"KAN\": kan_model,\n",
    "    \"MLP\": mlp_model,\n",
    "    \"KAC_Net\": kac_model,\n",
    "    \"KAL_Net\": kal_model\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Explaining model: {model_name}\")\n",
    "    shap_values, all_data = explain_model_with_shap_kernel(model, val_loader)\n",
    "    \n",
    "    # Plot the SHAP values\n",
    "    shap.summary_plot(shap_values, all_data, feature_names=['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
